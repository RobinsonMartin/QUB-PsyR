[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site",
    "section": "",
    "text": "This site contains resources for the upskilling of staff from QUB’s School of Psychology in R and RStudio.\n\nThis site is work in progress. I will continuously add or update content\n\nContributors: - Thomas Schultze-Gerlach (t.schultze(at)qub.ac.uk)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Space for QUB-Psy",
    "section": "",
    "text": "Welcome to the website. I hope you enjoy it!"
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "Correlations",
    "section": "",
    "text": "If we want to test whether there is a linear relation between two continuous variables \\(x\\) and \\(y\\), we can compute the correlation coefficient \\(r\\) as a measure of the strength of that relation. The correlation coefficient is defined as follows:\n\\(r_{x,y} = \\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{\\sigma_x\\sigma_y}\\)\nAs we can see, the correlation coefficient is the standardised covariance of \\(x\\) and \\(y\\). The standardisation (dividing the covariance by the product of the standard deviations of \\(x\\) and \\(y\\)) ensures that - unlike the covariance - the correlation coefficient is limited to the range from -1 to 1. Here, -1 indicates a perfect negative linear relationship, 1 indicates a perfect positive linear relationship, and 0 means that there is no linear relationship between \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "inference.html#computing-correlations-in-r",
    "href": "inference.html#computing-correlations-in-r",
    "title": "Correlations",
    "section": "Computing correlations in R",
    "text": "Computing correlations in R\nWe can compute the correlation coefficient \\(\\rho\\) in R using the function cor. We can use the cor function in two ways:\nThe first way is to feed the function two numerical vectors of the same length as two separate function arguments, x and y. R will then compute the correlation between x and y. Here is what the syntax looks like:\n\n# create a numeric vector\nv1 = -10:10\n\n# create a second numeric vector (square of v1)\nv2 = v1^2\n\n# compute the correlation of v1 and v2\ncor(x = v1, y = v2)\n\nIf we run this code, R will return the correlation coefficient a single number in the console (see blow).\n\n\n\n[1] 0\n\n\n\nIn our example, the correlation is zero despite y being a function of x (\\(y = x^2\\)). The reason is that the relationship of x and y is non-linear.\nThe second way, in which we can use the cor function is to feed it a numeric matrix (or data frame that contains only numeric variables) as the sole function argument x. If we do that, R will correlate each column of the matrix (or data frame) with all other columns - including itself - and create a correlation matrix. Here is an example.\n\n# create three numeric vectors\nv1 = 1:10     # just the numbers from 1 to 10\nv2 = sqrt(v1) # the square root of v1\nv3 = log(v1)  # the natural logarithm of v1\n\n# generate a numeric 10x3 matrix from the\n# three vectors using the cbind function\nm1 = cbind(v1, v2, v3)\n\n# compute the correlation matrix for m1\ncor(x = m1)\n\nRunning this code will lead R to return a numeric matrix containing the correlations in the console. Since we fed the cor function a matrix with three columns, the output will be a \\(3\\times3\\) matrix (see below).\n\n\n\n          v1        v2        v3\nv1 1.0000000 0.9891838 0.9516624\nv2 0.9891838 1.0000000 0.9861685\nv3 0.9516624 0.9861685 1.0000000\n\n\n\nThree things about the correlation matrix are noteworthy:\n\nIn the diagonal of the matrix, each correlation is 1. This makes sense because in the diagonal, we correlate each variable with itself.\nThe number above the diagonal mirror those below it. This make sense, too. The correlation of v1 and v2 is the same as the correlation of v2 and v1 (the order of the variables does not matter when computing a correlation).\nSpecific to our example, correlations between the three variables are close to perfect even though their relationships are non-linear. This shows how good linear approximations may be in some cases even thought the the assumption of a linear relationship is technically wrong."
  },
  {
    "objectID": "inference.html#rank-order-correaltions",
    "href": "inference.html#rank-order-correaltions",
    "title": "Correlations",
    "section": "Rank order correaltions",
    "text": "Rank order correaltions\nThe default correlation coefficient we can compute with the cor function is the product-moment correlation (as defined formally above). However, the cor function also allows us to compute rank order correlations if we so desire. Rank order correlations are robust against outliers, which makes them preferable to product-moment correlations in some situations (they are also sometimes referred to as non-parametric correlations).\nIn order to change the type of correlation coefficient, we can specify the function argument method when calling the cor function. This argument has a default called “pearson”, which computes the product-moment correlation. If we instead change it to “kendall” or “spearman”, R will instead compute Kendall’s \\(\\tau\\) or Spearman’s \\(\\rho\\), both of which are rank order correlations."
  },
  {
    "objectID": "inference.html#how-to-handle-missing-data",
    "href": "inference.html#how-to-handle-missing-data",
    "title": "Correlations",
    "section": "How to handle missing data",
    "text": "How to handle missing data\nWhen we want to compute correlation coefficients using the cor function, we need to make sure that there are no missing values in the objects we feed the function. Otherwise, R will return NA whenever one of the contribution observations is NA. In order to compute valid correlation coefficient, we need to use the function argument use. The default value is “everything”, nut if some of our observations are NA we don’t want to use everything. We have two options: “complete.obs” and “pairwise.complete.obs”.\nIf we want to compute the correlation of two variables, both options do the same: they remove all cases in which there is a missing value before computing the correlation coefficient.\nIf we want R to compute correlation matrix for three or more variables instead, the two values for use differ slightly. Using “complete.obs” will prompt R to remove all cases with at least one NA in any of the variables. This will ensure equal sample sizes for all computed correlation coefficient but may result in an unnecessary loss of data. For example, when a person has a missing in only one variable, we can still use their data to compute correlations between the remaining variables.\nIf we use “pairwise.complete.obs” instead, R will only exclude cases with an NA for the computation of those correlation coefficients which involve the missing response. That means, we use as many observations as possible to compute each correlation at the risk of creating slight imbalances between correlation coefficients regarding their underlying sample."
  },
  {
    "objectID": "inference.html#testing-for-significant-correlations",
    "href": "inference.html#testing-for-significant-correlations",
    "title": "Correlations",
    "section": "Testing for significant correlations",
    "text": "Testing for significant correlations\nSo far, we have only computed correlation coefficients. However, most of the time, we will also want to know whether the correlations in our data are so strong that we can reject the Null hypothesis that there is no linear relationship between the variables.\nWe can test for statistical significance using the function cor.test. This function takes two numeric vectors of the same length as function arguments x and y. It also automatically removes cases with NAs.\nJust as with the cor function, we can specify the type of correlation coefficient we want to test for significance using the function argument method. As with the cor function, the default is “pearson”, but we can change it to “kendall” or “spearman” if we want.\nSince direction matters when dealing with correlations, we can also specify the type of our alternative hypothesis using the function argument alternative. The default is “two.sided”, which tests whether the correlation is different from zero. We can change this argument to “greater” or “less” to test the directional hypotheses that r is positive or that it is negative, respectively.\nLet’s look at an example, in which we ant to run a two-tailed significant test on a product-moment correlation. Here is what the code would look like:\n\n# create two numeric vectors\nv1 = 1:10     # just the numbers from 1 to 10\nv2 = exp(v1)  # e to the power of v1 (because why not)\n\n# test of significant correlation of v1 and v2\ncor.test(x = v1, y = v2)\n\nRunning this code will prompt R to return a lot if information in the console. Here is that the output looks like:\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  v1 and v2\nt = 2.9082, df = 8, p-value = 0.01964\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.159019 0.927748\nsample estimates:\n      cor \n0.7168704 \n\n\n\nFrom this output, we can tell:\n\nthe type of correlation we tested, namely the product-moment correlation (makes sense because this default,a nd we did not change it)\nwhat data the correlation test is based on (v1 and v2)\nthat our test was two-tailed because our alternative hypothesis was that \\(r\\) is non-zero (also a default we did not change)\nthe test statistics of the underlying \\(t\\)-test, that is, the empirical \\(t\\) value, its degrees of freedom, and the \\(p\\)-value\nthe correlation coefficient and a 95% confidence interval (we could change the confidence level to something else using the conf.level argument; 0.95 is the default)\n\nIn the example above, the \\(p\\)-value of the test justifies rejecting the Null hypothesis.\nLet’s look at another example, in which we run a one-tailed test of significance on Spearman’s \\(\\rho\\), testing whether the correlation is greater than zero. Code as follows:\n\n# create two numeric vectors\nv1 = c(11, 14, 15, 10, 9, 4, 7, 8, 17, 6)\nv2 = c(12, 15, 17, 18, 5, 6, 9, 14, 8, 11)\n\n# test of significant correlation of v1 and v2\ncor.test(x = v1, y = v2, method = \"spearman\",\n         alternative = \"greater\")\n\nThe output in the console looks slightly different, mainly because we tested a rank order correlation (see below).\n\n\n\n\n    Spearman's rank correlation rho\n\ndata:  v1 and v2\nS = 102, p-value = 0.1395\nalternative hypothesis: true rho is greater than 0\nsample estimates:\n      rho \n0.3818182 \n\n\n\nAs we can see, R again tells us which type of correlation we tested, and our alternative hypothesis looked like. Since we tested Spearman’s \\(\\rho\\), the underlying statistical test is not a \\(t\\)-test (remember that this is a non-parametric correlation). Accordingly, R will show us a different test statistic names \\(S\\) and the corresponding \\(p\\)-value. Finally, R will tell us the correlation coefficient, but there will be no 95% confidence interval (R cannot compute it without assuming an underlying parametric function).\nIn this second example, Spearman’s \\(\\rho\\) is not significantly greater than zero. Thus, we have to concede that the result is uninformative."
  },
  {
    "objectID": "inference0.html",
    "href": "inference0.html",
    "title": "The (frequentist) Inference Game",
    "section": "",
    "text": "Before we have a look at specific types of statistical analyses, let’s take a step back and remember what we learned about frequentist statistics when we were undergraduate students."
  },
  {
    "objectID": "inference0.html#the-general-logic-of-frequentists-tests",
    "href": "inference0.html#the-general-logic-of-frequentists-tests",
    "title": "The (frequentist) Inference Game",
    "section": "The general logic of frequentists tests",
    "text": "The general logic of frequentists tests\nIn frequentist statistics, our inferences follow the same logic, irrespective of the kind of test we run. Specifically, we:\n\nchoose a test that is appropriate for the data (e.g., a \\(\\chi^2\\)-test for the analysis of frequencies, or an independent samples \\(t\\)-test for comparing continuous data between two groups)\ndefine a Null hypothesis \\(H_0\\) (usually that the effect is zero) and an alternative hypothesis \\(H_1\\) (usually that the effect is non-zero)\nchoose a tolerated type I error level \\(\\alpha\\) (usually 5%)\nrun the test and inspect the \\(p\\)-value it returns\ntinker with the data and analyses until \\(p\\) drops below .05\nmake a statement about the effect of interest based on the \\(p\\)-value\n\nThe last part is the tricky bit because many people seem to forget that there is an asymmetry in how informative results are when \\(p < .05\\) as opposed to \\(p > .05\\). Let’s have a look at them separately."
  },
  {
    "objectID": "inference0.html#inference-when-p-.05",
    "href": "inference0.html#inference-when-p-.05",
    "title": "The (frequentist) Inference Game",
    "section": "Inference when p < .05",
    "text": "Inference when p < .05\nIf the \\(p\\)-value of our test is smaller than our tolerated \\(\\alpha\\) level (e.g., .05), we consider the effect so be statistically significant. The reason is that the chance of obtaining a result at least as extreme as the one we observed is reasonably low if the effect did, in fact, not exist and \\(H_0\\) were true. Therefore, we accept \\(H_1\\), which states that the effect exists.\nLet’s paraphrase that: If p <.05, we decide to believe that the empirical result cannot be a chance finding, all the while knowing that it can be a chance finding! We even know what the probability of encountering that chance finding are if \\(H_0\\) is true, namely our tolerated \\(\\alpha\\)-level.\nIn essence, we know that there are two possible scenarios that can result in a statistically significant test result:\n\nThe effect does not exist, but we drew an unrepresentative sample.\nThe effect does exist, and the sample accurately represents it.\n\nHowever, if we accept the fact that we may sometimes (namely in \\(\\alpha\\)% of cases) make a mistake when interpreting significant findings as evidence of \\(H_1\\) being true, we can essentially get rid of option 1 when interpreting our results.\nThis laves us with the following interpretations whenever \\(p < .05\\):\n\nThe effect does not exist, but we drew an unrepresentative sample.\nThe effect does exist, and the sample accurately represents it.\n\nAccepting that we may sometimes err (without knowing where we erred), we have now created a situation, in which a statistically significant test result is unambiguous, that is, it has only one valid conclusion: the effect of interest exists (\\(H_0\\) is true)."
  },
  {
    "objectID": "inference0.html#inference-when-p-.05-1",
    "href": "inference0.html#inference-when-p-.05-1",
    "title": "The (frequentist) Inference Game",
    "section": "Inference when p > .05",
    "text": "Inference when p > .05\nLet’s now turn to the other side of the coin, namely situations, in which \\(p > .05\\). Our intuition may tell us that we can conclude that \\(H_1\\) must be wrong and that we can accept \\(H_0\\) to be true. This would lead us to state that the effect we were investigating does not exist. However, this case is a bit more complicated.\nIf we want to conclude that an effect does not exist, we must consider the type II error (overlooking an effect that actually exists). Logic would state that if we can reject \\(H_0\\) when we tolerate a certain type I error level (e.g., \\(\\alpha = .05\\)), then we can also reject \\(H_1\\) if we tolerate the same type II error level (i.e., \\(\\beta = .05\\)).\nSo far so good. The problem is that the type II error level depends on the true size of the effect we are studying, and we do not know the true effect size (if we did, we would not have to run a test, in the first place).\nWhat complicates things further is that the classic \\(H_1\\) (the effect is non-zero) is a composite hypothesis. What does that mean? It means that \\(H_1\\) comprises an infinite number of specific hypotheses, let’s call them \\(H_{1_{i}}\\), which differ from each other regarding the assumed true effect size. If any of the \\(H_{1_{i}}\\) is true, then \\(H_1\\) as a whole is true.\nIf we run a statistical test on our data, there will be some \\(H_{1_{i}}\\) postulating a true effect size that is large enough for the associated type II error level to be smaller than the one we are willing to tolerate. However, there will always be some of \\(H_{1_{i}}\\), for which the postulated effect size so small that the associated type II error level exceeds the one we are willing to accept.\nWhat this means is that we have three possible scenarios that can result in a statistically non-significant test result:\n\nThe effect does not exist, and the sample accurately represents it.\nThe effect exists, and its size is so large that the associated type II error level is below the level we are willing to tolerate.\nThe effect exists, but its size is so small that the associated type II error level exceeds the level we are willing to tolerate.\n\nUsing the same logic as above, we can eliminate option 2 because we are willing to accept a certain number of errors during our academic career. However, we cannot eliminate option 3 in the same fashion because the associated type II error chance is higher than what we are willing to tolerate. That leaves us with two possible scenarios:\n\nThe effect does not exist, and the sample accurately represents it.\nThe effect exists, and its size is so large that the associated type II error level is below the level we are willing to tolerate.\nThe effect exists, but its size is so small that the associated type II error level exceeds the level we are willing to tolerate.\n\nBecause we do not know which of the remaining two scenarios led to our non-significant result, we must acknowledge that it is uninformative. We cannot decide whether the effect is actually zero or whether it is non-zero but too small for us to detect reliably. In other words, when interpreting a non-significant result, we must not conclude that we have shown the effect no to exist. Instead, our answer needs to be: we don’t know (yet) whether the effect exists or not.\n\nThere is an elegant solution to the problem of interpreting non-significant \\(p\\)-values. We can determine the effect size \\(x\\) for which the type II error \\(\\beta\\) is equal to our tolerated type I error level \\(\\alpha\\).\nWe can then state - with the same confidence with which we decide to reject \\(H_0\\) if \\(p < .05\\) - that there is no effect greater than or equal to \\(x\\).\nHow useful that statement is, critically depends on the sample size of our study. With small samples, we may only be able to rule out huge effects. However, when our sample size is large enough, we may get to the point where we can rule out any non-trivial effect (what this means is that effect sizes we cannot rule out with the required confidence are too small to be of practical concern)."
  },
  {
    "objectID": "inference1.html",
    "href": "inference1.html",
    "title": "The t-Test",
    "section": "",
    "text": "The \\(t\\)-test is a parametric test designed to test whether the mean of a (approximately) normally distributed continuous variable \\(X\\) with unknown variance \\(\\sigma^2\\) differs from a certain fixed value \\(\\mu\\), for example, zero.\nThe test statistic \\(t\\) is formally defined as follows:\n\\(t = \\frac{\\bar{X}-\\mu}{\\hat{\\sigma}\\sqrt{n}}\\)\nHere, \\(\\bar{X}\\) is the empirically determined sample mean of the variable \\(X\\), \\(\\mu\\) is the assumed population parameter under \\(H_0\\), \\(\\hat{\\sigma}\\) is the estimate of the population standard deviation derived from the sample, and \\(n\\) is the size of the sample the test is based on.\nLike many statistical test, the \\(t\\)-test is a signal-to-noise ratio. The numerator represents the signal (i.e., the effect we are interested in) while the denominator represents the noise (based on the sampling variance).\nThe \\(t\\)-test comes in three flavours:\nThe only difference between the three types of \\(t\\)-tests is the definition of \\(\\bar{X}\\) and \\(\\sigma^2\\). For one-sample \\(t\\)-tests \\(X\\) is the variable we are measuring, and \\(\\sigma^2\\) is its variance.\nFor the other two types of \\(t\\)-test, \\(X\\) is the difference between two variables \\(Y\\) and \\(Z\\). Depending on the type of \\(t\\)-test, \\(Y\\) and \\(Z\\) are either two variables measured in the same group or one variable measured in two independent groups. In both cases, \\(\\sigma^2\\) is the variance of the difference \\(Y-Z\\).\nWe can run all three types of \\(t\\)-tests in R using the function t.test. This function is quite flexible. It has several function arguments that we can use to specify the type of \\(t\\)-test and how our \\(H_0\\) and \\(H_1\\) should look like. Let’s have a look at the most important function arguments:\nNow that we have a rough understanding of what we can do with the t.test function, let’s give it a spin."
  },
  {
    "objectID": "inference1.html#the-one-sample-t-test",
    "href": "inference1.html#the-one-sample-t-test",
    "title": "The t-Test",
    "section": "The one-sample t-test",
    "text": "The one-sample t-test\nFor starters, we will consider the most simple case, namely the one-sample \\(t\\)-test. For example, we could test whether the mean of a vector of numbers differs from zero.\n\nWe can generate some normally distributed data for our examples using the rnorm function. We simply need to feed the function three arguments: the size of the simulated sample (n), the true mean (mean), and the true standard deviation (sd). Calling the rnorm function will then result in R producing a pseudo-random sample drawn from the specified normal distribution.\nIf we want to create a reproducible pseudo-random sample, we can call the function set.seed before calling rnorm. If we feed the set.seed function an arbitrary number, the following pseudo-random draw will always be the same.\n\nHere is what the R code would look like:\n\n# create some normally distributed data using the rnorm function.\n# use set.seed to make sure the outcome of the data simulation is reproducible\nset.seed(1234) \n\n# simulate 50 data points drawn from a normal distribution with mean = 0.50 and sd = 1\nv1 = rnorm(n = 50, mean = 0.50, sd = 1)\n\n# run a one-sample t-test to test if the mean of v1 differs from zero\nt.test(x = v1)\n\nAs we can see, the code for the two-tailed one-sample t-test is very simple. Now let’s have a look at what appears in the console if we run the code above.\n\n\n\n\n    One Sample t-test\n\ndata:  v1\nt = 0.37508, df = 49, p-value = 0.7092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2045796  0.2984736\nsample estimates:\n mean of x \n0.04694699 \n\n\n\nThe output of the \\(t\\)-test function is actually a list (we can easily verify that by saving the output of the test as a new object). R displays the information contained in that list in an easily readable fashion in the console. In particular, R will tell us:\n\nthe type of \\(t\\)-test we ran (one-sample \\(t\\)-test)\nthe empirical \\(t\\)-value\nthe test’s degrees of freedom (n-1)\nthe \\(p\\)-value\nthe type of \\(H_1\\)\nthe mean of \\(X\\) and a 95% confidence interval around it\n\nIn the example above, the \\(p\\)-value is greater than the conventional significance level of .05. Accordingly, we cannot accept \\(H_1\\) and have to retain \\(H_0\\).\nLet’s now consider a second example, in which we want to test whether the mean of a numeric vector exceeds 50.\n\n# generate normally distributed data (n = 100) with true mean = 60 and and sd = 10 (make the draw reproducible using set.seed)\nset.seed(2345)\nv2 = rnorm(n = 100, mean = 60, sd = 10)\n\n# run a one-tailed one-sample t-test testing whether the mean of v2 exceeds 50\nt.test(x = v2, mu = 50, alternative = 'greater')\n\nLet’s have a look at the output in the console:\n\n\n\n\n    One Sample t-test\n\ndata:  v2\nt = 11.147, df = 99, p-value < 2.2e-16\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 58.77243      Inf\nsample estimates:\nmean of x \n 60.30786 \n\n\n\nThe output states correctly that we tested the a directional alternative hypothesis and what value we tested against (\\(H_1: \\mu > 50\\)). Due to the directional nature of the test, the 95% confidence interval around the mean is one-tailed, too (its upper limit is “Inf”, meaning infinite).\nOf course, R also tells us the \\(t\\)-value, degrees of freedom, and \\(p\\)-value of the test. This time, we can reject \\(H_0\\) and accept \\(H_1\\), accordingly."
  },
  {
    "objectID": "inference1.html#the-paired-t-test",
    "href": "inference1.html#the-paired-t-test",
    "title": "The t-Test",
    "section": "The paired t-test",
    "text": "The paired t-test\nLet’s now turn to cases, in which we want to test for the difference between two variables measured in the same group. This could be two variables measured at the same time, or the same variable measured at two times.\nIn order to run a paired \\(t\\)-test, we need to feed the t.test function two numerical vectors of the same length, x and y. R will pair each element of the first vector with the corresponding element of the second vector. Therefore, we need to ensure that the data for both vectors are in the same order (usually not a problem if we use data frames).\nLet’s look at an example, in which we want to test whether the stress level of 100 simulated people decreases between two measurement occasions T1 and T2.\n\nSince we want to simulate correlated data, using the rnorm function won’t help us. However, there is a useful function called mvrnorm in the MASS package. This function allows us to generate data from multivariate normal distributions.\nWe can feed the mvrnorm the following arguments:\n\nn: the number of observations\nmu: a vector of length k containing the means of the generated variables\nSigma: the \\(k\\times k\\) variance-covariance matrix\n\nThe function output will be a \\(n \\times k\\) matrix with each column containing the simulated data of one variable.\n\nHere is the code for the simulated data:\n\n# load the package MASS for its function mvrnorm\nlibrary(MASS)\n\n# generate a data frame containing data for 100 simulated people who reported their stress levels twice\n# In this example, we will set the true stress level at T1 to 0.50, the one at T2 to 0.10, and the correlation of the to measurements to r = 0.50\n# we also use set.seed for reproducibility\nset.seed(3456)\nmy_df = data.frame(\n  ID = 1:100,\n  stress = mvrnorm(\n    n = 100,\n    mu = c(0.50, 0.10),\n    Sigma = matrix(c(1, 0.50, \n                     0.50, 1), nrow = 2)\n  )\n)\n\n# assign better names to the two stress variables\nnames(my_df)[2:3] = c('stress_T1', 'stress_T2')\n\nWe can now run the paired \\(t\\)-test. Since we want to run a one-tailed test, we need to think carefully about how to specify our alternative hypothesis. Our hypothesis states that stress at T2 should be lower than stress at T1. If we enter stress at T1 first, R will compute \\(X\\) by subtracting T2 stress levels from T1 stress levels. If our hypothesis is correct, the difference of means should be positive. Therefore, we need to define the alternative hypothesis as “greater” (if we were to enter stress at T2 first, we would need to specify it as “less”, accordingly. Here is what the code looks like:\n\n# test whether there is a significant decrease in stress form T1 to T2\nt.test(x = my_df$stress_T1, \n       y = my_df$stress_T2, \n       alternative = 'greater',\n       paired = T)\n\nHere is the console output:\n\n\n\n\n    Paired t-test\n\ndata:  my_df$stress_T1 and my_df$stress_T2\nt = 3.5033, df = 99, p-value = 0.0003458\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 0.2114204       Inf\nsample estimates:\nmean difference \n      0.4019068 \n\n\n\nR will now inform us that we ran a paired \\(t-test\\) (this is correct as we set the paired argument to “paired”; otherwise, R would have computed and independent samples \\(t\\)-test).\nIt also tells us which alternative hypothesis we tested, namely that the difference between T1 and T2 stress levels was positive (greater than 0). Furthermore, R reports the mean difference, the one-tailed 95% confidence interval around it.\nFinally, it shows is the empirical \\(t\\)-value, degrees of freedom (n-1, similar to the one-sample test), and the \\(p\\)-value. In this example, stress was significantly lower at T2than at T1, allowing us to accept \\(H_1\\).\n\nThe statistically well-versed reader may interject that a paired-sample \\(t\\)-test is the same as a one-sample \\(t\\)-test on the difference of the two variables involved. That is technically correct!\nWe can easily verify it by computing a new variables as the difference between the two variables we want to compare. First, we run a paired-sample \\(t\\)-test by feeding the t.test function both variables. Then, we run a one-sample \\(t\\)-test and feed it only the new difference variable. The results of the two tests will be identical, ceteris paribus."
  },
  {
    "objectID": "inference1.html#independent-samples-t-test",
    "href": "inference1.html#independent-samples-t-test",
    "title": "The t-Test",
    "section": "Independent samples t-test",
    "text": "Independent samples t-test\nWe will now turn to the last version of \\(t\\)-test, the \\(t\\)-test for independent samples. As with the paired \\(t\\)-test, we need to feed the t.test function two numeric vectors containing the data we want to compare. However, since the data are assumed to be independent, the two vectors need not be of the same length, and the order of the elements does not matter.\nFirst, we are going to simulate a new data frame containing the data from 100 participants who were randomly assigned to two experimental conditions labelled “experimental” and “control” for lack of imagination. The only measured variable of interest is the dependent variable “dv”.\n\n# generate data for 100 participants, half of which \n# were assigned to an experimental condition while\n# the other half was assigned to the control group\n# we use set.seed for reproducible simulation\nset.seed(4567)\n\nmy_df2 = data.frame(\n  ID = 1:100,\n  condition = rep(c('experimental', 'control'), each = 50),\n  dv = c(\n    rnorm(n = 50, mean = 0.50, sd = 1), # experimental group data\n    rnorm(n = 50, mean = 0, sd = 1)     # control group data\n  )\n)\n# The function rep repeats all elements of the vector\n# a number of time specified by the argument 'each'\n\nNow that we have a proper simulated data set, we can run the independent samples \\(t\\)-test. Let’s assume we want to test whether the groups differ, that is, our \\(H_1\\) is unspecific, and we run a two-tailed \\(t\\)-test on the data. Here is what the code looks like:\n\n# test the hypothesis that the mean of dv differs\n# between the experimental and the control group\nt.test(x = my_df2$dv[my_df2$condition == 'experimental'],\n       y = my_df2$dv[my_df2$condition == 'control'],\n       paired = FALSE, var.equal = FALSE)\n\nTechnically, we do not need to specify the paired and var.equal argument because FALSE is the default for both. Let’s have alook at the output.\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  my_df2$dv[my_df2$condition == \"experimental\"] and my_df2$dv[my_df2$condition == \"control\"]\nt = 3.7368, df = 94.704, p-value = 0.0003186\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.3261383 1.0654884\nsample estimates:\nmean of x mean of y \n0.7206913 0.0248780 \n\n\n\nR tells us that we ran a Welch two-sample \\(t\\)-test. That is R’s way of letting us know that it corrected the test’s degrees of freedom for unequal variances. The standard \\(t\\)-test for independent samples has \\(n-2\\) degrees of freedom, so in our example, we would expect 98. However, due to slight differences in the empirically obtained variances between the two groups, the correction scales the degrees of freedom down to 94.704 (if you see a \\(t\\)-test with fractional degrees of freedom somewhere you now know what happened). The rest of the information is similar to what we saw in the previous examples. In our case, the data support accepting \\(H_1\\). If, for some reason, we do not want R to correct for unequal variances, we can set the var.equal argument to TRUE. Depending on how strong the inequality of variances is, this might have an impact on the \\(p\\)-value. Let’s give it a try with the data above.\n\n# test the hypothesis that the mean of dv differs\n# between the experimental and the control group\nt.test(x = my_df2$dv[my_df2$condition == 'experimental'],\n       y = my_df2$dv[my_df2$condition == 'control'],\n       paired = FALSE, var.equal = TRUE)\n\nHere is the console output:\n\n\n\n\n    Two Sample t-test\n\ndata:  my_df2$dv[my_df2$condition == \"experimental\"] and my_df2$dv[my_df2$condition == \"control\"]\nt = 3.7368, df = 98, p-value = 0.0003133\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.3262991 1.0653276\nsample estimates:\nmean of x mean of y \n0.7206913 0.0248780 \n\n\n\nAs we can see, R now states that we ran a “two sample \\(t\\)-test”. The degrees of freedom amount to the expected 98, and the \\(p\\)-values differs slightly from that of the Welch \\(t\\)-test.\nIf we run an independent samples \\(t\\)-test and our grouping variable (in our example the variable “condition”) has only two levels, we can use an alternative syntax containing a formula.\nAn R formula is a social type of R object that requires a specific syntax. Generally speaking this syntax looks like this:\n\n# generic formula stating that y is a function of x\ny ~ x\n\nWhat this means is that y is a function of x. Usually, R accepts only a single variable to the left of the tilde operator (~), whereas we can combine multiple variables on it’s right side. We will return to formulae later when we discuss linear models. For now the simple version is sufficient.\nIn case of an independent \\(t\\)-test, we can feed the t.test function a formula containing the variable names as well as another function argument called data, which requires a data frame as input. Here, we can tell R which data frame our variables are contained in. Let’s have a look at the alternative syntax for our test above.\n\n# test the hypothesis that the mean of dv differs\n# between the experimental and the control group\nt.test(dv ~ condition, data = my_df2,\n       paired = FALSE, var.equal = FALSE)\n\nAs we can see below, the output is the same as for the initial Welch \\(t\\)-test above. Which type of syntax we use depends on our personal preferences.\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  dv by condition\nt = -3.7368, df = 94.704, p-value = 0.0003186\nalternative hypothesis: true difference in means between group control and group experimental is not equal to 0\n95 percent confidence interval:\n -1.0654884 -0.3261383\nsample estimates:\n     mean in group control mean in group experimental \n                 0.0248780                  0.7206913 \n\n\n\nCongratulations, you are now able to do \\(t\\)-tests of all flavours in R!"
  },
  {
    "objectID": "inference2.html",
    "href": "inference2.html",
    "title": "The Chi² Test",
    "section": "",
    "text": "The \\(\\chi^2\\)-test allows us to test for deviations between observed and expected frequencies. The \\(\\chi^2\\) distribution is formalised as follows:\n\\(\\chi^2 = \\frac{(o_i - e_i)^2}{e_i}\\)\nHere, \\(o_i\\) denotes the observed frequencies in category \\(i\\) whereas \\(e_i\\) represents the expected frequencies given some assumed distribution. In brief, the larger the discrepancies between the observed and expected frequencies, the larger the \\(\\chi^2\\)-value becomes. If it exceeds a certain threshold (defined by the degrees of freedom of the specific test), we reject the Null hypothesis and decide to believe that the deviations of the observed from the expected frequencies are systematic.\nThe most common analyses for which the \\(\\chi^2\\)-test is used are:\nIf we want to run any of these \\(\\chi^2\\)-tests, we can use an R function called chisq.test. Which function argument the chisq.test function needs depends on what we want to do. Therefore, we will look at each case separately in the following."
  },
  {
    "objectID": "inference2.html#test-of-observed-frequencies-against-chance",
    "href": "inference2.html#test-of-observed-frequencies-against-chance",
    "title": "The Chi² Test",
    "section": "Test of observed frequencies against chance",
    "text": "Test of observed frequencies against chance\nThis is the simplest form of the \\(\\chi^2\\)-test. To run this test, we need only feed the chisq.test function a single function argument x, namely a numeric vector of length 2 or greater, containing the frequencies we want to analyse.\nThe function has another argument called p which specifies the expected probabilities against which to test the observed frequencies. The default value of this argument is equi-probability. That is, if we do not specify it, R will automatically test whether the observed frequencies systematically differ from a chance distribution.\nHere is an example of how the syntax for this test looks:\n\n# create a vector containing frequencies for three categories\nv1 = c(19, 33, 21)\n\n# test for deviation from equal probabilities\nchisq.test(x = v1)\n\nIf we run the syntax above, R will conduct the \\(\\chi^2\\)-test by comparing the observed frequencies shown in v1 against chance (expected frequencies are computed using a probability of one third for each of the three categories). The output looks as follows:\n\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  v1\nX-squared = 4.7123, df = 2, p-value = 0.09478\n\n\n\nAs we can see, R tells us which object it ran the test on (v1). It also tells us the empirical \\(\\chi^2\\) value (named X-squared), the degrees of freedom of this test (2 because v1 has three categories), and the associated \\(p\\)-value. In our example, and given that we run the test using the conventional \\(\\alpha\\) level of 5%, the differences in the observed frequencies do not warrant rejecting the Null hypothesis. That is, we cannot state that some of the categories in v1 are more likely than others."
  },
  {
    "objectID": "inference2.html#test-of-observed-frequencies-against-a-predefined-probability",
    "href": "inference2.html#test-of-observed-frequencies-against-a-predefined-probability",
    "title": "The Chi² Test",
    "section": "Test of observed frequencies against a predefined probability",
    "text": "Test of observed frequencies against a predefined probability\nLet’s assume that we want to compare v1 not against chance but against unequal probabilities for the three categories. For example, our Null hypothesis could state that the first and second category should occur in 25% of cases each while category three should account for the remaining 50%. To run this test, the only change we need to make to the way we called the chisq.test function in the example above is to specify the function argument p and define it as a numeric vector containing the probabilities.\nThis vector of probabilities needs to be of the same length as the vector of observed frequencies. In addition, since we are dealing with probabilities, all elements need to range between 0 and 1, and their sum must equal 1. Let’s have a look at the syntax.\n\n# test for deviation from pre-defined non-equal probabilities\nchisq.test(x = v1, p = c(0.25, 0.25, 0.50))\n\nThe output of the test looks very much like the one in the previous example. Again, R shows us the empirical \\(\\chi^2\\)-value, the degrees of freedom, and the \\(p\\)-value (see below).\n\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  v1\nX-squared = 18.534, df = 2, p-value = 9.448e-05\n\n\n\nThis time, the empirical \\(\\chi^2\\) value is greater, and the corresponding p-value is small enough to reject the Null hypothesis. That is, we can state that v1 was not drawn from a population, in which category 3 is twice as frequent as categories 1 and 2."
  },
  {
    "objectID": "inference2.html#tests-for-systematic-covariation-in-contengency-tables",
    "href": "inference2.html#tests-for-systematic-covariation-in-contengency-tables",
    "title": "The Chi² Test",
    "section": "Tests for systematic covariation in contengency tables",
    "text": "Tests for systematic covariation in contengency tables\nIn some cases, we may not be interested in comparing observed frequencies against a (theoretical) distribution, but rather to compare observed frequencies across two variables (i.e., in a two-dimensional contingency table). Conceptually, this boils down to testing - row-by-row or column-by-column - whether the underlying distribution of one variable is equal across all levels of the other variable.\nTo do so, we need to feed the chisq.test function a matrix of frequencies as the function argument x. We no longer need to specify the function argument p because the expected frequencies are computed from the marginal frequencies of the contingency table assuming stochastic independence.\nLets look at an example. First, we need some data in table format. We can create such data using the function matrix or by combining several vectors of equal length using either the function cbind or rbind (these functions stack equally long vectors of the same type by column or row). Let’s go with the rbind version for now.\n\n# create a 2x3 contingency table by stacking \n# two vectors containing frequencies row-wise\ndata1 = rbind(c(762, 327, 68), \n        c(484, 239, 77))\n\n# run a chi² test on the data\nchisq.test(data1)\n\nRunning the code above will perform the \\(\\chi^2\\)-test on the contingency table we created. Here is what the data looks like.\n\n\n\n     [,1] [,2] [,3]\n[1,]  762  327   68\n[2,]  484  239   77\n\n\n\nThe output in the console when running the \\(\\chi^2\\)-test looks as follows:\n\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data1\nX-squared = 11.525, df = 2, p-value = 0.003143\n\n\n\nOnce more, R tells us the empirical \\(\\chi^2\\)-value, the degrees of freedom (for \\(n\\times m\\) tables, the test has \\(n-1\\times m-1\\) degrees of freedom, which makes 2 in our example), and the \\(p\\)-value of the test. From the \\(p\\)-value, we can see that there is systematic co-variation, that is, some cells are more and others less frequent than we would expect if the two variables were unrelated."
  },
  {
    "objectID": "inference2.html#extracting-frequencies-from-data-frames",
    "href": "inference2.html#extracting-frequencies-from-data-frames",
    "title": "The Chi² Test",
    "section": "Extracting frequencies from data frames",
    "text": "Extracting frequencies from data frames\nThe data we want to feed into \\(\\chi^2\\)-tests is usually contained in data frames. We can extract frequencies and even turn them into contingency tables using the table function. Let’s consider an example, in which we have data from one thousand simulated nerds participants who told us their attitudes toward Star Trek and Star Wars (love, hate, indifferent).\n\n\n\nLet’s say the data is contained in a data frame called my_df. We can use the function head to inspect the first six rows of the data frame to get a feeling for what the data looks like.\n\n# inspect the top of the data frame\nhead(my_df)\n\nHere is what appears in the consle:\n\n\n\n  ID    starTrek    starWars\n1  1 indifferent        love\n2  2        hate indifferent\n3  3 indifferent        love\n4  4        hate        love\n5  5        love indifferent\n6  6        love        love\n\n\n\nWe can now create a contingency table using the table function and define it as a new object. To this end, we need to feed the table function the two dimensions by which we want to organise the table as separate function arguments. Here is what the code looks like:\n\n# create a 3x3 contingency table of attitudes toward\n# Star Trek and Star Wars\nmy_table = table(my_df$starTrek, my_df$starWars)\n\n# assign dimension names to the table (Star Trek goes first because\n# we entered it first into the table function above)\n# this step is optional but it makes the table more readable\ndimnames(my_table) = list(StarTrek = c('hate', 'indifferent', 'love'),\n                          StarWars = c('hate', 'indifferent', 'love'))\n\nOnce we have defined the new object, we can inspect the contingency table in the console by running its name as R code. Here is what it looks like:\n\n\n\n             StarWars\nStarTrek      hate indifferent love\n  hate          28          49   71\n  indifferent   44          99  142\n  love          81         212  274\n\n\n\nNow that we have created the contingency table from the data frame, we can feed it into the chisq.test function as its sol argument to test whether there is systematic co-variation between attitudes toward Star trek and Star Wars. Here is what the code looks like.\n\nchisq.test(my_table)\n\nHere is the console output that results from running the test using the syntax above:\n\n\n\n\n    Pearson's Chi-squared test\n\ndata:  my_table\nX-squared = 2.5325, df = 4, p-value = 0.6388\n\n\n\nRunning the test shows us the usual information in the console. In this specific example, the \\(p\\)-value does not allow rejecting the Null hypothesis. In other words, there is no evidence in our simulated data indicating that whether one’s attitude toward Star Trek is in any way related on one’s attitude toward Star Wars."
  },
  {
    "objectID": "inference3.html",
    "href": "inference3.html",
    "title": "Correlations",
    "section": "",
    "text": "If we want to test whether there is a linear relation between two continuous variables \\(x\\) and \\(y\\), we can compute the correlation coefficient \\(r\\) as a measure of the strength of that relation. The correlation coefficient is defined as follows:\n\\(r_{x,y} = \\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{\\sigma_x\\sigma_y}\\)\nAs we can see, the correlation coefficient is the standardised covariance of \\(x\\) and \\(y\\). The standardisation (dividing the covariance by the product of the standard deviations of \\(x\\) and \\(y\\)) ensures that - unlike the covariance - the correlation coefficient is limited to the range from -1 to 1. Here, -1 indicates a perfect negative linear relationship, 1 indicates a perfect positive linear relationship, and 0 means that there is no linear relationship between \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "inference3.html#computing-correlations-in-r",
    "href": "inference3.html#computing-correlations-in-r",
    "title": "Correlations",
    "section": "Computing correlations in R",
    "text": "Computing correlations in R\nWe can compute the correlation coefficient \\(\\rho\\) in R using the function cor. We can use the cor function in two ways:\nThe first way is to feed the function two numerical vectors of the same length as two separate function arguments, x and y. R will then compute the correlation between x and y. Here is what the syntax looks like:\n\n# create a numeric vector\nv1 = -10:10\n\n# create a second numeric vector (square of v1)\nv2 = v1^2\n\n# compute the correlation of v1 and v2\ncor(x = v1, y = v2)\n\nIf we run this code, R will return the correlation coefficient a single number in the console (see blow).\n\n\n\n[1] 0\n\n\n\nIn our example, the correlation is zero despite y being a function of x (\\(y = x^2\\)). The reason is that the relationship of x and y is non-linear.\nThe second way, in which we can use the cor function is to feed it a numeric matrix (or data frame that contains only numeric variables) as the sole function argument x. If we do that, R will correlate each column of the matrix (or data frame) with all other columns - including itself - and create a correlation matrix. Here is an example.\n\n# create three numeric vectors\nv1 = 1:10     # just the numbers from 1 to 10\nv2 = sqrt(v1) # the square root of v1\nv3 = log(v1)  # the natural logarithm of v1\n\n# generate a numeric 10x3 matrix from the\n# three vectors using the cbind function\nm1 = cbind(v1, v2, v3)\n\n# compute the correlation matrix for m1\ncor(x = m1)\n\nRunning this code will lead R to return a numeric matrix containing the correlations in the console. Since we fed the cor function a matrix with three columns, the output will be a \\(3\\times3\\) matrix (see below).\n\n\n\n          v1        v2        v3\nv1 1.0000000 0.9891838 0.9516624\nv2 0.9891838 1.0000000 0.9861685\nv3 0.9516624 0.9861685 1.0000000\n\n\n\nThree things about the correlation matrix are noteworthy:\n\nIn the diagonal of the matrix, each correlation is 1. This makes sense because in the diagonal, we correlate each variable with itself.\nThe number above the diagonal mirror those below it. This make sense, too. The correlation of v1 and v2 is the same as the correlation of v2 and v1 (the order of the variables does not matter when computing a correlation).\nSpecific to our example, correlations between the three variables are close to perfect even though their relationships are non-linear. This shows how good linear approximations may be in some cases even thought the the assumption of a linear relationship is technically wrong."
  },
  {
    "objectID": "inference3.html#rank-order-correaltions",
    "href": "inference3.html#rank-order-correaltions",
    "title": "Correlations",
    "section": "Rank order correaltions",
    "text": "Rank order correaltions\nThe default correlation coefficient we can compute with the cor function is the product-moment correlation (as defined formally above). However, the cor function also allows us to compute rank order correlations if we so desire. Rank order correlations are robust against outliers, which makes them preferable to product-moment correlations in some situations (they are also sometimes referred to as non-parametric correlations).\nIn order to change the type of correlation coefficient, we can specify the function argument method when calling the cor function. This argument has a default called “pearson”, which computes the product-moment correlation. If we instead change it to “kendall” or “spearman”, R will instead compute Kendall’s \\(\\tau\\) or Spearman’s \\(\\rho\\), both of which are rank order correlations."
  },
  {
    "objectID": "inference3.html#how-to-handle-missing-data",
    "href": "inference3.html#how-to-handle-missing-data",
    "title": "Correlations",
    "section": "How to handle missing data",
    "text": "How to handle missing data\nWhen we want to compute correlation coefficients using the cor function, we need to make sure that there are no missing values in the objects we feed the function. Otherwise, R will return NA whenever one of the contribution observations is NA. In order to compute valid correlation coefficient, we need to use the function argument use. The default value is “everything”, nut if some of our observations are NA we don’t want to use everything. We have two options: “complete.obs” and “pairwise.complete.obs”.\nIf we want to compute the correlation of two variables, both options do the same: they remove all cases in which there is a missing value before computing the correlation coefficient.\nIf we want R to compute correlation matrix for three or more variables instead, the two values for use differ slightly. Using “complete.obs” will prompt R to remove all cases with at least one NA in any of the variables. This will ensure equal sample sizes for all computed correlation coefficient but may result in an unnecessary loss of data. For example, when a person has a missing in only one variable, we can still use their data to compute correlations between the remaining variables.\nIf we use “pairwise.complete.obs” instead, R will only exclude cases with an NA for the computation of those correlation coefficients which involve the missing response. That means, we use as many observations as possible to compute each correlation at the risk of creating slight imbalances between correlation coefficients regarding their underlying sample."
  },
  {
    "objectID": "inference3.html#testing-for-significant-correlations",
    "href": "inference3.html#testing-for-significant-correlations",
    "title": "Correlations",
    "section": "Testing for significant correlations",
    "text": "Testing for significant correlations\nSo far, we have only computed correlation coefficients. However, most of the time, we will also want to know whether the correlations in our data are so strong that we can reject the Null hypothesis that there is no linear relationship between the variables.\nWe can test for statistical significance using the function cor.test. This function takes two numeric vectors of the same length as function arguments x and y. It also automatically removes cases with NAs.\nJust as with the cor function, we can specify the type of correlation coefficient we want to test for significance using the function argument method. As with the cor function, the default is “pearson”, but we can change it to “kendall” or “spearman” if we want.\nSince direction matters when dealing with correlations, we can also specify the type of our alternative hypothesis using the function argument alternative. The default is “two.sided”, which tests whether the correlation is different from zero. We can change this argument to “greater” or “less” to test the directional hypotheses that r is positive or that it is negative, respectively.\nLet’s look at an example, in which we want to run a two-tailed significance test on a product-moment correlation. Here is what the code would look like:\n\n# create two numeric vectors\nv1 = 1:10     # just the numbers from 1 to 10\nv2 = exp(v1)  # e to the power of v1 (because why not)\n\n# test of significant correlation of v1 and v2\ncor.test(x = v1, y = v2)\n\nRunning this code will prompt R to return a lot if information in the console. Here is that the output looks like:\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  v1 and v2\nt = 2.9082, df = 8, p-value = 0.01964\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.159019 0.927748\nsample estimates:\n      cor \n0.7168704 \n\n\n\nFrom this output, we can tell:\n\nthe type of correlation we tested, namely the product-moment correlation (makes sense because this default,a nd we did not change it)\nwhat data the correlation test is based on (v1 and v2)\nthat our test was two-tailed because our alternative hypothesis was that \\(r\\) is non-zero (also a default we did not change)\nthe test statistics of the underlying \\(t\\)-test, that is, the empirical \\(t\\) value, its degrees of freedom, and the \\(p\\)-value\nthe correlation coefficient and a 95% confidence interval (we could change the confidence level to something else using the conf.level argument; 0.95 is the default)\n\nIn the example above, the \\(p\\)-value of the test justifies rejecting the Null hypothesis.\nLet’s look at another example, in which we run a one-tailed test of significance on Spearman’s \\(\\rho\\), testing whether the correlation is greater than zero. Code as follows:\n\n# create two numeric vectors\nv1 = c(11, 14, 15, 10, 9, 4, 7, 8, 17, 6)\nv2 = c(12, 15, 17, 18, 5, 6, 9, 14, 8, 11)\n\n# test of significant correlation of v1 and v2\ncor.test(x = v1, y = v2, method = \"spearman\",\n         alternative = \"greater\")\n\nThe output in the console looks slightly different, mainly because we tested a rank order correlation (see below).\n\n\n\n\n    Spearman's rank correlation rho\n\ndata:  v1 and v2\nS = 102, p-value = 0.1395\nalternative hypothesis: true rho is greater than 0\nsample estimates:\n      rho \n0.3818182 \n\n\n\nAs we can see, R again tells us which type of correlation we tested, and our alternative hypothesis looked like. Since we tested Spearman’s \\(\\rho\\), the underlying statistical test is not a \\(t\\)-test (remember that this is a non-parametric correlation). Accordingly, R will show us a different test statistic named \\(S\\) and the corresponding \\(p\\)-value. Finally, R will tell us the correlation coefficient, but there will be no 95% confidence interval (R cannot compute it without assuming an underlying parametric function).\nIn this second example, Spearman’s \\(\\rho\\) is not significantly greater than zero. Thus, we have to concede that the result is uninformative."
  },
  {
    "objectID": "inference4.html",
    "href": "inference4.html",
    "title": "The Ananlysis of Variance",
    "section": "",
    "text": "The Analysis of Variance (ANOVA) is a generalisation of the \\(t\\)-Test. It is used to test the difference between two or more means against zero. When comparing groups using ANOVA, we partition the total variance inherent to the data (collapsed across all groups) into the variance between groups (i.e., the variance of the group means) and the variance within groups."
  },
  {
    "objectID": "inference4.html#the-foundations-of-anova",
    "href": "inference4.html#the-foundations-of-anova",
    "title": "The Ananlysis of Variance",
    "section": "The foundations of ANOVA",
    "text": "The foundations of ANOVA\nA key concept in ANOVA is that of the sum of squares. To understand the sum of squares, let us consider the formal definition of the variance \\(\\sigma^2\\) of a variable, which looks as follows:\n\\[\\sigma^2_x = \\frac{1}{n} \\times \\underbrace{\\Sigma{(x_i - \\bar{x})^2}}_\\text{sum of squares}\\]\nFrom this equation, we can see that the variance of the variable \\(x\\) is the product of two components. One is the sum of squares, that is, the sum of the squared deviations of each observation \\(x_i\\) from their mean \\(\\bar{x\\)}. The second factor simply divides this sum of squares by the number of observations \\(n\\).\nWhen we estimate the true variance \\(\\sigma^2_x\\) from data we collected, using the formula above produces a slight bias. Specifically, our estimate of the true variance will be systematically too low. To arrive at an unbiased estimate, we need to divide the um of squares by \\(n-1\\) instead of \\(n\\), leading to the following formal definition of the estimate of the variance \\(\\hat{\\sigma}^2_x\\):\n\\[\\hat{\\sigma}^2_x = \\frac{1}{n-1}\\Sigma{(x_i - \\bar{x})^2}\\]\n\nFun fact: Dividing a sum of squares by its degrees of freedom yields the so called mean squares. The mean squares follow a \\(\\chi^2\\)-distribution with the respective degrees of freedom."
  },
  {
    "objectID": "inference4.html#the-logic-of-the-anova",
    "href": "inference4.html#the-logic-of-the-anova",
    "title": "The Ananlysis of Variance",
    "section": "The logic of the ANOVA",
    "text": "The logic of the ANOVA\nThe whole idea of ANOVA rests on the fact that the variance can be partitioned into the the sum of its parts. This is also true for the the sum of squares. As a very basic rule, we can state that:\n\\[SS_{total} = SS_{between} + SS_{within}\\]\nHere, \\(SS_{total}\\) is a measure of the total variability of a variable, \\(SS_{between}\\) represents that part of the total variability that is due to differences of the group means, and \\(SS_{within}\\) represents the part of the total variability that results from heterogeneity within the groups.\nSimilar to the \\(t\\)-test, ANOVA is a signal to noise ratio, where we treat variability between groups as the signal and variability within groups as the noise. The test statistic we use to test for significant mean differences between groups is the \\(F\\)-value, which is formally defined as:\n\\[F = \\frac{VAR_{between}}{VAR_{within}}=\\frac{\\frac{SS_{between}}{df_{between}}}{\\frac{SS_{within}}{df_{within}}} = \\frac{MS_{between}}{MS_{within}}\\]\nIn the formula above, \\(MS\\) is the corresponding mean squares.\n\nFun fact: Based on the formula above, we can easily derive that \\(F\\) is the ratio of two \\(\\chi^2\\)-distributed variables. Both the numerator and the denominator constitute a sum of squares divided by its degrees of freedom. That is also the reason why the \\(F\\)-distribution has two degrees of freedom, one for the numerator and one for the denominator.\n\nHow we compute the mean squares depends on the type of ANOVA we run and the number of groups we compare in it.\nHowever, we can state generally how we compute the different sum of squares. First of all, let’s remember how we compute the total sum of squares, \\(SS_{total}\\):\n\\[SS_{total} = \\sum_{i=1}^{N} (x_i - \\bar{x})^2\\]\nThe total sum of squares represent the sum of the squared deviation of all \\(N\\) observed data points from the grand mean \\(\\bar{x}\\). Now lets look at the formula for \\(SS_{between}\\).\n\\[SS_{between} = \\sum_{j=1}^{J} n_j \\times (\\bar{x_j} - \\bar{x})^2\\] Here, \\(J\\) is the number of groups we compare, and \\(n_j\\) is the sample size of group \\(j\\). For the \\(SS_{between}\\) we pretend that there is no variance within the \\(J\\) groups at all. Each observation is represented by its group’s mean \\(\\bar{x_j}\\), and we compute the variability as the difference of these group means from the grand mean \\(\\bar{x}\\). Therefore, the \\(SS_{between}\\) isolates the between-group part of the total variability of \\(x\\). Let’s now turn to the \\(SS_{within}\\).\n\\[SS_{within} = \\sum_{j=1}^{J} \\sum_{k = 1}^{n_j} (x_{jk} - \\bar{x_j})\\] Again, \\(J\\) is the number of groups we compare, and \\(n_j\\) is the sample size in group \\(j\\). The \\(x_{jk}\\) refers to the \\(k\\)th observation in group \\(j\\). For the \\(SS_{within}\\), we pretend that there is no variance between groups at all. We do so by substituting the grand mean for the respective group means \\(\\bar{x_j}\\). Thus, the \\(SS_within\\) isolates the within-group variability of \\(x\\).\nThe final thing we need to understand before we can delve into the actual ANOVAs is the \\(F\\)-statistic. As we have seen above, we compute \\(F\\) as a ratio of the \\(MS_{between}\\) and the \\(MS_{within}\\). This ratio is interesting in several ways:\n\nSince the \\(SS_{between}\\) usually has much fewer degrees of freedom than that \\(SS_{within}\\), the variability between groups does not have to be nearly as large as the variability within groups to produce a large \\(F\\)-value.\nThe more groups we compare, the lower the \\(F\\)-ratio will be, ceteris paribus. However, this does not necessarily mean that it becomes more difficult to detect significant mean differences. The more groups we compare, and the more numerator degrees of freedom our test has, the lower the critical \\(F\\)-value past which we consider a result statistically significant.\nThe larger our total sample, the more denominator degrees of freedom we have, and smaller the noise becomes in our signal-to-noise ratio, ceteris paribus. This makes intuitive sense: as the samples size increases, our measurement becomes more precise, making it easier to detect differences between group means.\n\n\nFun fact: If we use an ANOVA to compare two means, we effectively run a \\(t\\)-test but discard its ability to indicate the direction of the effect.\nIn such cases \\(F = t^2\\), and the \\(p\\)-value of both tests will be identical if we run a \\(t\\)-test assuming equal variances.\n\nANOVAs come in various flavours. In the following, we will look at some of them, namely:\n\none-factorial ANOVAs (between-subjects)\ntwo-factorial ANOVAs (between-subjects)\nrepeated-measures ANOVAs (within-subjects)\nmixed ANOVAs (at least one between and within factor)"
  },
  {
    "objectID": "inference4.html#one-factorial-anova",
    "href": "inference4.html#one-factorial-anova",
    "title": "The Ananlysis of Variance",
    "section": "One-factorial ANOVA",
    "text": "One-factorial ANOVA\nIn a one-factorial ANOVA, we compare two or more group means such that we consider each group to represent one level of the same grouping variable or factor. The Null hypothesis is that all group means are equal, and the alternative hypothesis is that not all group means are equal.\nRunning ANOVAs in base R tends to be very clunky because R is more centered around classic regression models. Therefore, we won’t be using base R to run ANOVAs, but instead use an R package called afex (Analysis of Factorial Experiment). That means, we need to install and load afex first.\n\n# install.packages(\"afex\")\nlibrary(afex)\n\n\n\n\nOnce we have done that, we can start doing ANOVAs with one of several functions:\n\naov_car\naov_4\naov_ez\n\nThe three functions serve the same goal and do the same things, but they differ in terms of the syntax. The function aov_car uses syntax that is most closely related the the (clunky) base R version of ANOVA. In contrast, aov_4 uses syntax based on the popular lme4 package that is widely used for the analysis of generalised mixed models. Thus, this function is ideally suited for users who are already familiar with lme4. Finally, aov_ez uses a completely string-based format, that is, it does not require a formula type object as a function argument. The advantage of aov_ez is that it is very convenient and easy to handle (thus the suffix “ez”). This comes at the cost of flexibility. The lack of a formula means that we are stuck with a full ANOVA model. The other two functions technically allow us to specify models that omit certain main effects or interactions. However, since we will rarely want to run these incomplete models, this is a drawback that usually causes no hassles. In the following, we will focus on the aov_ez function and leave exploration of the other functions to the discretion of the reader,\nHere are the most important function arguments of aov_ez:\n\nid (necessary): a character value indicating the name of the variable that contains our subject ID\ndv (required): another character value; the name of the variable containing the data the group means of which we want to compare\ndata (required): an R object of type data frame containing the data we want to analyse\nbetween (optional): a character string or character vector indicating the name(s) of the variable(s) constituting the between-subjects factor(s) of our design; default is NULL meaning that there are no between-subjects factors\nwithin (optional): a character string or character vector indicating the name(s) of the variable(s) constituting the within-subjects factor(s) of our design; default is NULL meaning that there are no within-subjects factors\ncovariate: a character value or vector indicating the name(s) of the covariate(s) in our analysis\nfactorize (optional): a logical value; determines if the between- and within-subject variables are turned into factors prior to the analysis; default is TRUE; if our design has at least one covariate, we need to set this to FALSE and make sure that all factors are defined as such manually\nanova_table (optional): a list of further arguments passed to the function; the ones we may be interested in are es (effect size; default is ‘ges’, which yields \\(\\eta^2\\) as an effect size measure, but we can switch it to ‘none’ or to ‘pes’, which yields \\(\\eta^2_p\\)) and correction (non-sphericity correction method; default is ‘none’, bu we can switch it to ‘GG’ for the Greenhouse-Geisser or ‘HF’ for the Huynh-Feldt correction)\n\nFor the purpose of running a one-factorial between-subjects ANOVA, we can disregard some of the function arguments shown above. The only ones we need are id, dv, between, and possibly anova_table in case we want to obtain the effect size.\n\nNow that we want to do ANOVAs, it is time to talk about factors. In R, factors are a special type of vector that contain both values and labels for those values. The different values of vectors are considered to be categories. Factors are important because the ANOVA-function we use here requires its between- and within-subject variables to be factors.\nWe can create a factor using the factor function by feeding it the following function arguments:\n\nx: a vector we want to turn into a factor\nlevels: a vector containing all possible values the factor can take\nlabels: a character vector assigning a label to each level of the factor\n\n\nLets look at an example, in which we test whether the means of three groups are equal or not. First, we need to create some data.\n\n# create a data frame containing data from 30 subjects in three groups of 10 each; here, \"id\" is the subject identifier, \"cond\" is the between-subjects grouping variable, and \"dv\" contains the outcome variable we want to compare between groups\n\nmy_df = data.frame(\n  # subject ID\n  ID = 1:30,      \n  # between-subjects-factor 'cond'\n  cond = factor(  \n    rep(x = 1:3, each = 10),\n    levels = 1:3,\n    labels = c('control', 'treatment1', 'treatment2')),\n  # outcome variable 'dv'\n  dv = c(\n    c(10, 12,  9, 14, 11, 15, 13, 15, 18, 12), # dv data for control\n    c( 9,  7, 15, 14,  8,  7, 16, 13, 11, 16), # dv data for treatment1\n    c(12, 11, 11,  9,  8, 13,  8,  6, 14,  7)) # dv data for treatment2\n)\n\nWe can inspect how the data frame looks using the head function.\n\n\n\n  ID    cond dv\n1  1 control 10\n2  2 control 12\n3  3 control  9\n4  4 control 14\n5  5 control 11\n6  6 control 15\n\n\n\nNow that we have some data, we can run the ANOVA. The syntax looks as follows:\n\naov_ez(id = 'ID', between = 'cond', dv = 'dv', data = my_df)\n\nHere is what the output in the cosole looks like:\n\n\n\nContrasts set to contr.sum for the following variables: cond\n\n\nAnova Table (Type 3 tests)\n\nResponse: dv\n  Effect    df  MSE    F  ges p.value\n1   cond 2, 27 9.27 2.44 .153    .106\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\nAs we can see, R displays an ANOVA table in the console along with some additional information. The output is preceded by a message (this is NOT an error message; the code ran properly). This message informs us that the aov_ez function set the contrast type for our factor ‘cond’ to ‘contr.sum’. What this means is that the contrast underling our factor was forced into effect coding because that is the format that ANOVAs use. We don’t need to concern ourselves with that.\nNext, R will tell us that this output is an ANOVA table based on type-3 sum of squares. Type-3 sums of squares are what most statistics packages use. If your knowledge of statistics is so advanced that you can make an informed decision that you would prefer type-2 sums of squares, you can change it by setting the type function argument to 2.\nNow for the important bits. R shows us what the response variable in our model is, namely ‘dv’. Below that information, it displays the ANOVA table. Because we ran a one-factorial ANOVA; this table contains only one row. Here, we can see the name of the between-subjects factor (effect), the numerator and denominator degrees of freedom for its \\(F\\)-value (df), the mean squares of the effect (MSE), the \\(F\\)-value, the generalised \\(\\eta^2\\) as a measure of the effect size, and the \\(p\\)-value.\nIn our example, the mean difference is not statistically significant, which mans that we cannot reject the null hypothesis. In other words, we cannot say whether the true means between the three groups differ.\n\nSimilar to the \\(\\chi^2\\)-test, the \\(F\\)-test we use in an ANOVA is always a one-tailed test because it is based on squared variables. Therefore, the test has no ‘direction’ as a \\(t\\)-test would.\nWhy is this important? Sometimes, we might encounter a scientific article, in which the authors state that they ran a ‘one-tailed’ ANOVA test, but what they do in those articles is simply the divide their \\(p\\)-value by 2. This practice (often encountered when the regular \\(p\\)-value lies between .05 and .10) rests on the erroneous belief that all statistical tests are - per default - two-tailed and can, therefore, also be run as a one-tailed test with slightly greater power."
  },
  {
    "objectID": "inference4.html#disentangling-significant-effects-in-anovas",
    "href": "inference4.html#disentangling-significant-effects-in-anovas",
    "title": "The Ananlysis of Variance",
    "section": "Disentangling significant effects in ANOVAs",
    "text": "Disentangling significant effects in ANOVAs\nIn the example above, we had to retain the Null hypothesis because the analysis did not show evidence that the three means were different from each other. In those cases, there is no need for further analyses. However, things look a bit different when the ANOVA yields a significant result. Let’s look at an example.\nIn this example, we will compare two treatments and one control condition with data for 30 participants in each condition. The data is stored in a data frame called my_df2. Here is what the data looks like (we use the function head on the data frame to have R show us the first 6 lines).\n\n\n\n  ID    cond  dv\n1  1 control  88\n2  2  treat1 119\n3  3  treat2 119\n4  4 control 103\n5  5  treat1 103\n6  6  treat2 137\n\n\n\nWe now run the ANOVA on the data. Other than before, we will define the result of the analysis as a new R object. This will make it easier for us to disentangle the effect later on. Here is the syntax:\n\n# run a one-factorial ANOVA and save its results as a new R object\nmodel1 = aov_ez(id = 'ID', dv = 'dv', between = 'cond', data = my_df2)\n\nWe will now see a new object called “model1” in the Environment. R will tell us that this object is a list. Entering the new object’s name as syntax will show us the result of the ANOVA just as if we had called the function as is instead of defining it as a new object. Here is the console output:\n\n\n\nAnova Table (Type 3 tests)\n\nResponse: dv\n  Effect    df   MSE         F  ges p.value\n1   cond 2, 87 89.48 22.91 *** .345   <.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\nAs we can see, the effect of cond (the grouping variable in our fictional data) is statistically significant. We can now state - with the usual confidence - that the means of the three groups are not equal. However, we cannot say anything else. Since an ANOVA uses the squared test statistic \\(F\\), we have no information on the direction of the mean differences, nor do we know which means differ. Thus, we need further analyses to get a clear picture of the mean differences the ANOVA detected.\n\nAs a useful - if somewhat brutish - metaphor, think of an ANOVA as firing a shotgun into think fog. A statistically significant effect means that we hit something, but we do not know what we hit. In order to find that out we need to venture into the fog and have a closer look.\n(Credit for this metaphor goes to Prof. Dieter Heyer)\n\nIn order to disentangle a significant effect in an ANOVA with three or more groups, we need to run post-hoc analyses. There are many different ways to run such post-hoc analyses. We will look at three of them here:\n\nPost-hoc t-tests\nPerhaps the easiest way to disentangle significant effects in an ANOVA is running \\(t\\)-tests to compare the means of the groups. We already know how to do this, namely by using the t.test function. The advantage, beyond the fact that it is easy, is that we can specify one-tailed \\(t\\)-tests if they are theoretically justified and, ideally, preregistered as follow-up tests in case an ANOVA yields a significant result.\nIn our specific example, let’s assume that we expected both treatments to be effective, which should manifest in higher mean scores as compare with the control condition. Let us further assume that we had no specific prediction regarding the relative effectiveness of the two treatments. In this case, we would run two one-tailed \\(t\\)-tests, one comparing the first treatment condition with the control condition, and another comparing the second treatment with the control condition. For the final post-hoc comparison, we would run a two-tailed \\(t\\)-test comparing the two treatment conditions.\nLets have a look at the syntax first. It could look like this:\n\n# one-tailed t-test, testing whether dv scores are higher in the first treatment condition than in the control condition\nt.test(x = my_df2$dv[my_df2$cond == 'treat1'],\n       y = my_df2$dv[my_df2$cond == 'control'],\n       alternative = 'greater')\n\n# one-tailed t-test, testing whether dv scores are higher in the second treatment condition than in the control condition\nt.test(x = my_df2$dv[my_df2$cond == 'treat2'],\n       y = my_df2$dv[my_df2$cond == 'control'],\n       alternative = 'greater')\n\n# two-tailed t-test, testing whether dv scores differ between the two treatments\nt.test(x = my_df2$dv[my_df2$cond == 'treat1'],\n       y = my_df2$dv[my_df2$cond == 'treat2'])\n\nHere is what R outputs in the console once we run the three tests:\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  my_df2$dv[my_df2$cond == \"treat1\"] and my_df2$dv[my_df2$cond == \"control\"]\nt = 2.1864, df = 57.789, p-value = 0.01642\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 1.232097      Inf\nsample estimates:\nmean of x mean of y \n102.36667  97.13333 \n\n\n\n    Welch Two Sample t-test\n\ndata:  my_df2$dv[my_df2$cond == \"treat2\"] and my_df2$dv[my_df2$cond == \"control\"]\nt = 6.6635, df = 57.541, p-value = 5.552e-09\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 12.13568      Inf\nsample estimates:\nmean of x mean of y \n113.33333  97.13333 \n\n\n\n    Welch Two Sample t-test\n\ndata:  my_df2$dv[my_df2$cond == \"treat1\"] and my_df2$dv[my_df2$cond == \"treat2\"]\nt = -4.3842, df = 57.951, p-value = 4.968e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -15.973871  -5.959462\nsample estimates:\nmean of x mean of y \n 102.3667  113.3333 \n\n\n\nAs we can see, all three tests show evidence of mean differences. That is, based on these post-hoc comparisons, we can now say why the ANOVA yielded a significant effect: all three means differ from each other.\nBut wait! Don’t we need to correct for multiple comparisons? The answer is: it depends on your testing philosophy. If you want to treat the complete post-hoc analysis as a single analysis, then yes, you need to control your type-I error. The easiest way to do this is to use Bonferroni’s correction, that is, we divide the tolerated type-I error level \\(\\alpha\\) by the number of tests we ran \\(k\\).\nIn our case, this means dividing the usual \\(\\alpha\\) level of .05 by three (the number of post-hoc tests we ran). We now consider a post-hoc \\(t\\)-test significant only if the associated \\(p\\)-value is smaller than .01667. In our example, correcting for multiple testing does not change the general pattern of results. All three tests still yield significant mean differences.\n\nIn this example, things would look differently, had we used two-tailed tests to compare the treatments with the control condition. Specifically, the comparison of the first treatment condition with the control condition would have yielded a significant result when not correcting for multiple comparisons (the tests then yields a \\(p\\)-value of .033).\nAfter using a Bonferroni correction, this comparison would no longer be statistically significant since the \\(p\\)-value of the two-tailed tests exceeds the adjusted \\(\\alpha\\)-level of .01667.\n\n\n\nTukey’s method for pairwise comparisons\nRunning pairwise comparisons using \\(t\\)-tests is fine if we compare only few groups. However, as the number of groups we compare in an ANOVA increases, so does the number of possible pairwise comparisons. In fact, the number of comparisons is \\((k^2-k)/2\\), that is, it grows exponentially.\nAn alternative to running individual tests is Tukey’s method, also known as Tukey’s honestly significant difference (HSD). In technical terms, HSD computes the critical mean difference for which a pairwise comparison is considered significant, and then judges each comparison by that difference. The test statistic \\(q\\) follows the Studentised range distribution, the shape of which depends on the number of groups \\(k\\) and the sample size \\(n\\) (via the distribution’s degrees of freedom). Because the distribution of the \\(q\\)-statistic considers the number of groups, the HSD adjusts for multiple comparisons.\nIf we want to run post-hoc comparisons using HSD in R, we need another package called emmeans. This package contains two functions we need: emmeans (yes, it has the same name as the package) and contrast.\nLet’s first look at the emmeans function. When we call this function, we need to specify two function arguments. The first is called object. Here, we need to feed the function an R object containing the results of an ANOVA (good thing we saved our ANOVA as an R object). Second, we need to tell the function the name of grouping variable in our data. We do so by defining the function argument spec as a character string equalling the name of the respective variable (in our example, the grouping variable is ‘cond’).\nThe second function, contrast also needs two function arguments to do what we want it to do. The first is, again, called object. However, the contrast function does not need an ANOVA type object. Instead, it needs the type of object that the emmeans function returns (it is called an “emmGrid” type object). As a second argument, we need to specify method, which determines the type of post-hoc comparisons the function computes. The default is “eff”, which tests each group against the mean of all groups, but this is not what we want R to do. In order to get the HSD comparisons, we need to set this argument to “tukey”.\nLet’s have a look at the code using the ANOVA model we saved as an R object above (“model1”). There are two ways of writing this code, one that creates a separate object of type “emmGrid” in the environment which we can feed to the contrast function, and another that omits creating this object by feeding the contrast function another function call as its object (the latter may be preferable because we won’t do much with an “emmGrid” type object).\n\n# uncomment to install the package emmeans\n# install.packages(emmeans)\n# load the library emmeans\nlibrary(emmeans)\n\n## Alternative 1: save the emmGrid object separately\n\n# create an emmGrid object from the ANOVA\ngrid1 = emmeans(object = model1, specs = 'cond')\n\n# obtain Tukey's HSD using the function \"contrast\"\ncontrast(object = grid1, method = 'tukey')\n\n## Alternative 2: don't create an emmGrid object\n\n# obtain Tukey's HSD using the function \"contrast\"\ncontrast(object = emmeans(object = model1, specs = 'cond'), \n         method = 'tukey')\n\nRunning either version of the code, will produce the following output in the console:\n\n\n\n contrast         estimate   SE df t.ratio p.value\n control - treat1    -5.23 2.44 87  -2.143  0.0872\n control - treat2   -16.20 2.44 87  -6.633  <.0001\n treat1 - treat2    -10.97 2.44 87  -4.490  0.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\nAs we can see, the output is a table that contains the result of the HSD post-hoc test. Here, R will tell us which groups were compared and what the estimated mean difference for the respective comparison is. It will also tell us the standard error of the mean differences and the degrees of freedom (total sample size \\(n\\) minus the number of groups \\(k\\)). Finally, there will be a \\(t\\)-value and and \\(p\\)-value for each of the pairwise comparisons.\nBased on the HSD post-hoc tests, we would conclude that scores are higher in treatment 2 than in both the control condition and treatment 1, but we cannot say whether scores in treatment 1 are different from those in the control condition.\n\n\nCustom post-hoc contrasts\nThe third way to run post-hoc tests we will consider here is to run custom \\(t\\)-contrasts. What this means is that we define ourselves how we want to compare the group means in case of a significant effect in the ANOVA. When using custom contrasts, we can choose how many comparisons we run, which means we want to compare, and how to compare them.\nWhat we are effectively doing here is to compute weighted means of our conditions such that the weights adds to zero. For example, we could compare two group means (just as we did with the two previously described methods), but we could also test whether two groups differ from a third.\nLet us go back to our ANOVA example and assume that we want to run two post-hoc tests. The first tests whether having any treatment leads to different scores than being in the control condition. The second tests whether the effectiveness of the two treatments differs. Here is what the two contrasts would look like:\n\n\n\n\ncontrol\ntreatment 1\ntreatment 2\n\n\n\n\ncontrast 1\n-1\n+0.5\n+0.5\n\n\ncontrast 2\n0\n1\n-1\n\n\n\nLet’s dissect that! For contrast 1, we compare the control group with the mean of the two treatment groups. In contrast 2, we compare only the two treatment groups; the 0 weight for the control condition means that its mean does not play any role in this contrast.\n\nIn a design with three or more groups, contrasts comparing two group means have a distinct advantage over run-of-the-mill \\(t\\)-tests that compare the same means.\nThe reason is that - assuming equal variances in all groups - information from any group can be used to estimate the variance of the mean difference. While the group means of those conditions that receive a contrast weight of zero play no role in determining the signal, we can use the data from these groups to reduce the noise in our test.\nTherefore, a \\(t\\)-contrast will usually be more powerful than a regular \\(t\\)-test, although this advantage decreases with the sample size.\n\nHow do we tell R that we want to run custom contrasts? We can do so using the same contrast function we used to run the HSD test. To run custom contrasts, we simply need to change how we specify the function’s method argument. To be exact, we need to define this argument as a list in which each element is one contrast.\nWhen defining the list of contrasts, each contrast must be a numeric vector of length \\(k\\), where \\(k\\) is the number of groups in the grouping variable that we fed into the emmeans function (in our case 3). If we chose the wrong vector length, we will receive an error message. We also need to make sure that the sum of each contrast vector is zero (R will not check this, and the function will work even if we input nonsensical weights). Finally, we can (but do not have to) create these vectors as named elements of the list. Naming the vectors may lead to output that is slightly easer to make sense of.\nHere, too, we need to decide whether we save the list of contrasts as a separate object or whether we feed the contrast function a call of the list function (the result will be the same, so it is simply a matter of preference). Now let’s look at the R code.\n\n## Alternative 1: separate objects\n\n# create an emmGrid object from the ANOVA\ngrid1 = emmeans(object = model1, specs = 'cond')\n\n# create a list containing the custom contrasts\nmy_contrasts = list(\n  treat_vs_control = c(-1, 0.5, 0.5),\n  treat1_vs_treat2 = c(0, 1, -1)\n)\n\n# run the custom post-hoc tests using \"contrast\"\ncontrast(object = grid1, method = my_contrasts)\n\n## Alternative 2: don't create separate objects\n\n# run the custom post-hoc tests using \"contrast\"\ncontrast(object = emmeans(object = model1, specs = 'cond'), \n         method = list(\n             treat_vs_control = c(-1, 0.5, 0.5),\n             treat1_vs_treat2 = c(0, 1, -1)\n             )\n         )\n\nRunning either version of the code will prompt R to test the contrasts, and we will receive the following console output:\n\n\n\n contrast         estimate   SE df t.ratio p.value\n treat_vs_control     10.7 2.12 87   5.066  <.0001\n treat1_vs_treat2    -11.0 2.44 87  -4.490  <.0001\n\n\n\nAs we can see, R returns a table with two contrast tests. Since we named the contrasts properly, it is easy to see what these contrasts tested. For each contrast, R will tell is the estimated mean difference, the associated standard error, and the degrees of freedom (\\(n-k\\), similar to the HSD test described above). It will also show us the \\(t\\)-value and \\(p\\)-value for each contrast. In our example, both contrasts are significant. This means, we can state - with the usual confidence - that the ANOVA was significant because a) getting a treatment leads to higher scores as compared with the control condition, and b) the treatments differ in effectiveness.\nBut what about multiple comparisons? So far, we have not corrected for multiple comparisons, but we can do so if we want to. The good news is that we do not have to do it manually when using the contrast function. Instead, we can specify a third function argument called adjust. If we use custom contrasts, the default for this argument is “none”, but we can set it to “bonferroni” (for the classic Bonferroni correction) or “holm” (for the slightly lees concervative Holm-Bonferroni method) instead. Let’s go with Bonferroni. Here is the code:\n\n# run the custom post-hoc tests using \"contrast\" and correct\n# for multiple comparisons using Bonferroni's method\ncontrast(object = emmeans(object = model1, specs = 'cond'), \n         method = list(\n             treat_vs_control = c(-1, 0.5, 0.5),\n             treat1_vs_treat2 = c(0, 1, -1)\n             ),\n         adjust = 'bonferroni'\n         )\n\nNow let’s look at the console output:\n\n\n\n contrast         estimate   SE df t.ratio p.value\n treat_vs_control     10.7 2.12 87   5.066  <.0001\n treat1_vs_treat2    -11.0 2.44 87  -4.490  <.0001\n\nP value adjustment: bonferroni method for 2 tests \n\n\n\nAs we can see, not much has changed in terms of statistical significance, which makes sense because the unadjusted \\(p\\)-values were already very low. However, R now tells us at the bottom of the table that \\(p\\)-values were adjusted using the Bonferroni method.\n\nCustom contrasts are really useful because of their flexibility. They allow us to test very specific hypotheses. In fact, if we can state in advance which means in our design should differ (for example, because we have a strong theory to base our hypotheses on), we can technically skip the ANOVA altogether and instead run only the contrast tests. In those cases, we speak of a-priori contrasts.\n\n\n\nA concluding remark on post-hoc comparisons\nWe started the journey into post-hoc analyses using the metaphor of the ANOVA as firing a shotgun into thick fog which - in case we hit something - necessitates wandering into the fog in order to find out what exactly we hit. As the examples above show, the answer to that crucial question depends on which post-hoc comparisons we run (pairwise comparisons vs. custom contrasts), the type of alternative hypotheses we test (one-tailed vs. two-tailed tests), and whether and how we control for type-I error inflation due to multiple comparisons.\nWhen trying to disentangle a significant effect in an ANOVA, we may find ourselves faced with multiple options, each of which is valid and can be argued for convincingly. However, even subtle differences between these options may lead to qualitatively different conclusions. The practical advice here is to preregister exactly which tests we will run in case an ANOVA yields a significant result (ideally by providing the R code for the post-hoc analysis)."
  },
  {
    "objectID": "intro1.html",
    "href": "intro1.html",
    "title": "R and RStudio",
    "section": "",
    "text": "R is a software for statistical analysis (among others), but we will not work with R directly. Instead, we will be using R via a software called Rstudio, which serves as a user interface. While there is nothing wrong with using R on its own, RStudio makes the life of the users easier."
  },
  {
    "objectID": "intro1.html#understanding-the-rstudio-interface",
    "href": "intro1.html#understanding-the-rstudio-interface",
    "title": "R and RStudio",
    "section": "Understanding the RStudio interface",
    "text": "Understanding the RStudio interface\nYou have got the latest version of R and RStudio on your computer and are eager to get things going? Great!\nIf you open R Studio for the first time, it will look like this:\n\n\n\nFig 1. RStudio interace when opening for the first time\n\n\nThe interface has three parts, the R console, the memory, and the utility and help area. Once you open your first R script (you can do do by clicking on “File”, selecting “New File”, and choosing “R Script”), a fourth (and probably the most important) appears in the top left, the script area (note; when you close all scripts, the script area will disappear again until a new script is opened).\n\n\n\nFig 2. RStudio interface when at least one script is open"
  },
  {
    "objectID": "intro1.html#the-rstudio-interface-components",
    "href": "intro1.html#the-rstudio-interface-components",
    "title": "R and RStudio",
    "section": "The RStudio interface components",
    "text": "The RStudio interface components\n\nThe console\nThe console is the direct link between RStudio and R. It executes the R code we enter, either manually or by running (part of) a script. With the exception of plots, the output of the code we entered, is also displayed in the console. We will be using the console mostly in the early states of getting used to working with R. We can execute code in the console by typing it in and confirming it via the “Enter” key. Doing so will directly produce the result of the code we entered (if we did it correctly) or an error message (if we managed to botch it up).\n\n\nThe memory\nThe memory area is one of the many advantages of using RStudio as compared to R only. It has different subsections. The subsection labeled Environment displays all the objects that are currently in R’s memory. Examples of such objects include variables we defined, data we read into R, or models we fit on our data. The list of objects in the Environment is rather handy because we do not need to memorize the names of all our objects, and we can even have a closer look at our more complex objects (e.g., we can have a look at the individual variables in our data object).\nThe second subsection is the History of our R session. The history contains all the code we sent to the console (irrespective of whether we did it by manually entering code in the console or by running a script). Using the History is handy if we want to trace back what we did, for example to search for errors.\n\n\nUtility and Help\nThe Utility and Help area located on the bottom right has five subsections, which can be tremendously helpful, so let’s have a closer look at them.\n\nFiles: This section displays the content of the current working directory. We can use it like the windows explorer to search for and load files.\nPlots: If we create plots, they will be displayed in this subsection.\nPackages: The base version of R comes with a wide range of functions that we can use to handle and analyse data. However, we will frequently want to do things that base R cannot do. In those situations, we will require additional R packages. The Packages section shows, which packages are installed on our computer, and which ones are currently active (more on that later).\nHelp: The Help section is very important. We can use it to obtain information on all functions and objects contained in base R and all currently active packages.\nViewer: We will rarely be using the Viewer when starting to learn R. It can be used to display local html-files that we can create using R.\n\n\n\nThe Script\nThe Script area in the top left of the interface is where we do most of our actual work. Here, we can create, save, load, and modify R scripts. If we open more than one script, each one will have its own tab displaying the name of the script (or labeling it as “unnamed” if we have not saved it yet). Clicking on the tabs lets us switch between scripts."
  },
  {
    "objectID": "intro2.html",
    "href": "intro2.html",
    "title": "Running code in R",
    "section": "",
    "text": "As mentioned before, the R console is where we execute code and where all the computing is done. Before we start using R scripts, we first need to get a rough understanding of what the R console does. Think of the console as a glorified calculator. You tell it to run a piece of code and then R returns the answer.\nThere are two ways to run code in R: entering it manually in the R console or writing a script and executing part or all of it."
  },
  {
    "objectID": "intro2.html#running-code-via-the-console",
    "href": "intro2.html#running-code-via-the-console",
    "title": "Running code in R",
    "section": "Running code via the console",
    "text": "Running code via the console\nWe can execute code by typing it in the console and hitting “Enter”. We will keep it simple for now. Lets say we want R to compute a few numbers. It could look like this:\n\n\n\nFig 1. Input and output of simple code in the R console\n\n\nAs we can see, each of the commands yields the appropriate answer. In theory, we could enter code that is much more complex, but entering long lines of code quickly becomes a hassle we would prefer to avoid. Also, once we close R we would have to enter the same code again manually, which is not only a lot of (unnecessary) work but also error prone.\n\nNote: Generally speaking, using the console directly to run code is not very sensible, the exception being queries using the help-function (more on the help-function later)."
  },
  {
    "objectID": "intro2.html#running-code-via-r-scripts",
    "href": "intro2.html#running-code-via-r-scripts",
    "title": "Running code in R",
    "section": "Running code via R scripts",
    "text": "Running code via R scripts\nThe most common way to run code in R is to write an R script and then execute it. The great advantage of an R script is that we can save it and re-run it as often as we want without having to enter everything by hand once again. If you are keen about Open Science, you can share the Script as part of your Open Analysis Code policy. Since R is free, researchers who want to look at, re-run, or even copy your code can do so without worrying about license fees.\nFor example, we can write a script containing the same computations we entered manually into the console above. To do so, we first need to create a new (end empty) R script by clicking on “File”, then on “New File” and selecting “R Script”.\n\n\n\nFig 2. Creating a new R script\n\n\nWe can now edit the script. Here, we enter each of the computations we want R to do as a single line of our script. Once all four lines are ready, we select them all (either by marking them with the mouse or by clicking ctrl+a). We then tell RStudio to run the selected code by clicking on the “Run”-button on the top right above the script or by pushing ctrl+Enter.\n\n\n\nFig 3. Running the code contained in the R script\n\n\nAs we can see, the output in the console looks as if we had manually entered the code and pressed “Enter” after each line. We could have obtained the same result by copy-pasting the content of the script into the console and hitting “Enter”.\nIn the image above, our script is yet unsaved as can be seen from its name “untitled1”. We can save it by clicking on the floppy disc Symbol (for those of you who still know what a 3.5” floppy disc is) or by clicking on “File” and then “Save” or “Save as”. Since we have not yet named our script, we will be prompted to select a name and to choose in which folder we want to save the script.\nFor example, we can name this script “my first script”. Once we have done that, the tab above the script shows the name of the script (or the start of the name if we chose a long name) followed by “.R” indicating that this is an R file.\n Also note how the name of the script is now shown in black font as opposed to the red font it had prior to saving. Writing a script’s name in red font is RStudio’s way of telling you that this script has unsaved changes."
  },
  {
    "objectID": "intro2.html#commenting-your-code",
    "href": "intro2.html#commenting-your-code",
    "title": "Running code in R",
    "section": "Commenting your code",
    "text": "Commenting your code\nOne crucial part of an R script are comments. Comments are used to explain the code we write. Commenting our scripts is very important as it provides structure to them and makes the code legible, be it for others who try to understand our code or to ourselves (trust me, you will learn to appreciate a well-commented code if you return to a script you wrote a few months back and cannot remember what you did there).\nWe can write comments by adding a # in front of the text. All text following a # in the same line will be ignored by R when running the script.\n\n\n\nFig 5. Running code with comments\n\n\nWhen running the script the whole code is copied to the console, but only the actual code is being evaluated. All the text that we declared as comments is being ignored.\n\nNote: A good rule of thumb is to have at least one line of comments for every four lines of code. When in doubt, opt for more or more detailed comments. Better to over-explain your code than to risk it being unintelligible."
  },
  {
    "objectID": "intro3.html",
    "href": "intro3.html",
    "title": "Objects and functions",
    "section": "",
    "text": "R revolves around objects and functions. Generally speaking, an object is a container for information, and a function is a piece of code that performs a specific task. We often use functions to manipulate or create objects."
  },
  {
    "objectID": "intro3.html#r-objects",
    "href": "intro3.html#r-objects",
    "title": "Objects and functions",
    "section": "R objects",
    "text": "R objects\nIn R, an object is information that we store in R’s memory. In order to define an R object, we need to choose a name and tell R what information the object should contain. The general syntax for defining objects looks like this:\n\nobject_name = information\n\nWe write the name of our new object to the left of an equal sign and define the information the object should contain to its right. Instead of using an equal sign to define an object, we can also draw an arrow using the less than sign and a hyphen, which looks like this:\n\nobject_name <- information\n\n\nUsing the arrow instead of an equal sign dates back to an old programming language that R is derived from (APL). Whether you define objects using arrows or equal signs has no effect on what R does. It is simply a matter of preference.\n\nAs soon as we have defined an object (and assuming that we did everything correctly), our new object will appear in the Environment tab of RStudio’s Memory section (top right panel).\nWe will now cover the most important of the basic R objects, namely:\n\nsingle values\nvectors\nmatrices\n\n\nWe will later turn toward two of the more complex R objects that we are very likely to encounter frequently when working with R:\n\ndata frames\nlists"
  },
  {
    "objectID": "intro3.html#functions-in-r",
    "href": "intro3.html#functions-in-r",
    "title": "Objects and functions",
    "section": "Functions in R",
    "text": "Functions in R\nR functions are predefined pieces of code that we can call by writing the function name and telling R a number of function arguments. Function arguments are pieces of information a function needs to know in order to perform its task (very rarely, a function may not need any argument to perform its task). The general syntax for calling a function looks like this:\n\nfunction_name(argument1, argument2, ...)\n\nThat is, we first write the function’s name, and then define all required arguments in parentheses. If we do that correctly, the function will perform its task. Depending on the function, we will see an output in the console.\nWe will be using two functions below:\n\nThe function c, which we will use to create vectors. The arguments the function c requires are the elements we want to combine into a vector.\nThe function matrix, which creates a matrix from a vector. This functions requires a vector, the number of rows and the number of columns as arguments.\n\n\nCore fact 1: Functions can have a lot of arguments, but it is not always necessary to specify all of them. Most functions have default values for some of their arguments. If we do not specify these arguments, R will just run the function as if we had entered the default value of the argument. If we do specify arguments with default values, we simply override the default.\n\n\nCore fact 2: It is impossible (for most people at least) to remember all functions and which arguments they require. Therefore, R contains a lifesaver in the form of a special function called help. The function help requires the name of a topic (including but not limited to functions) as its main argument. Calling help with the name of a function will show the documentation for that function in the Help tab of the Utility & Help section of RStudio’s interface (bottom right)."
  },
  {
    "objectID": "intro3.html#object-types",
    "href": "intro3.html#object-types",
    "title": "Objects and functions",
    "section": "Object types",
    "text": "Object types\nSingle values, vectors, and matrices can vary by type. The type of these objects refers to the nature of its content, for example whether it contains numbers, character strings, logical arguments etc. The most common types of types are:\n\ndouble: this means numbers with decimal points (although the decimals must not necessarily be displayed)\ninteger: whole numbers that can be negative positive or zero (rarely used because double type numbers are more practical)\ncharacter: a character string, which is essentially a sequence of text symbols (these symbols can include numbers, but these are interpreted as being part of the text and not as numbers)\nlogical: a logical TRUE or FALSE, also called a boolean value"
  },
  {
    "objectID": "intro3.html#single-values",
    "href": "intro3.html#single-values",
    "title": "Objects and functions",
    "section": "Single values",
    "text": "Single values\nSingle values are the simplest type of objects we can define in R. As their name suggests, they represent a single value, for example, the number 7. We could tell R to create a single value called a that consists of the number 7. We do this as follows:\n\na = 7\n\nAlternatively, we can use the arrow notation to define the object.\n\na <- 7\n\nOnce we run either line of code, we should notice that our Environment now contains the object a. There, we can also see that a is the number 7.\nWe can make R show us an object by running its name as code (either via the console or a script). If we enter the name of our single value a as code, we will see the following output.\n\n\n\n[1] 7\n\n\n\nR tells us “7”, which is exactly the value that we specified as a. The “[1]” in front of the value “7” is not important at this point. It merely tells us that the “7” is the first (and only) element of our single value."
  },
  {
    "objectID": "intro3.html#vectors",
    "href": "intro3.html#vectors",
    "title": "Objects and functions",
    "section": "Vectors",
    "text": "Vectors\nVectors are objects that contain multiple values of the same type. Vectors cannot contain elements of different types. We can define vectors using the function c with the function’s arguments being the values we want to combine into the respective vector. The syntax looks as follow:\n\nv1 = c(1,2,3)   # This line of code creates a double vector \n                # called \"v1\" containing the numbers 1 to 3.\n\nv2 = c('hello', 'purple', '11!')  # This creates a character vector\n                                  # called \"v2\" containing three strings.\n\nAfter executing this code, our Environment contains two more objects, v1 and v2. The vector v1 is denoted as a numeric vector (num) whereas v2 is labeled as a character vector (chr).\nJust as with the single value a, we can have a look at our vectors by entering their names as code. For example, we can enter v2 as code.\n\n\n\n[1] \"hello\"  \"purple\" \"11!\"   \n\n\n\nWe now see the three character strings that v2 consists of in the console. Again, there is a [1] at the front of the line. It indicates that “hello” is the first element of v2. If the vector was long enough to occupy multiple rows in the console, each line would start with a number in brackets, indicating which element of the vector is displayed at the beginning of that line.\n\nFun fact: We can think of single values as vectors of length 1.\n\n\nCaveat: If we try to create a vector containing different types of elements, R will not complain (i.e., we do not get an error message)! Instead, R will simply transform some of the elements to ensure that all elements share the same type.\nFor example, if at least one of the elements we want to include in the vector is a character string, R will transform all other element types to character strings and create a character vector.\nIf we try to combine numeric (double or integer) and logical values into one vector, R will interpret “FALSE” as 0 and “TRUE” as 1."
  },
  {
    "objectID": "intro3.html#matrices",
    "href": "intro3.html#matrices",
    "title": "Objects and functions",
    "section": "Matrices",
    "text": "Matrices\nMatrices are the two-dimensional cousins of vectors. They have r rows and c columns. Per convention, the number of rows is stated before the number of columns, that is, a 4x3 matrix has 4 rows and 3 columns (think roman-catholic as a mnemonic aid).\nAs with vectors, all elements of a matrix must share the same type (if we try to combine different types of elements, R will just transform some of the elements to create a homogeneous matrix). In fact, matrices in R are vectors that we break down into rows and columns. To create a matrix, we need to call the function matrix and tell R three arguments: the vector we want to make into a matrix, the number of its rows, and the number of its columns. The syntax looks as follows:\n\nmatrix1 = matrix( # this tells R that we want to create a matrix\n  c(1,2,3,4),     # defines the vector that will turn into a matrix\n  nrow = 2,       # this tells R that our matrix should have 2 rows \n  ncol = 2)       # this tells R that our matrix should have 2 columns\n\nExecuting the code above creates a numerical matrix in our Environment. Other than with single values and vectors, we can click on the name of our matrix in the Environment to view how it looks like. Alternatively, we can enter the name of our matrix as code and have R print it in the console.\n\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n\n\nR is convenient in the sense that it is technically sufficient to specify either the number of rows or the number of columns. R will just infer the missing piece of information. For example, if we want to turn a vector of length 6 into a matrix with 2 columns, R will know that the matrix must have 3 rows."
  },
  {
    "objectID": "intro3.html#naming-objects-in-r",
    "href": "intro3.html#naming-objects-in-r",
    "title": "Objects and functions",
    "section": "Naming objects in R",
    "text": "Naming objects in R\nIn order to define an object in R, we need to give it a name and then tell R how the object should look like. Generally speaking, we can get very creative when naming our objects, but there are a few rules we need to keep in mind.\n\nObject names must not start with numbers. The name “item2” is valid, but “2nd” is not (it will return a error message complaining about an “unexpected symbol”).\nObject names may not contain special characters with the exception of underscores and periods. For example, the names “item_2” or “item.2” are valid. Using other special character will often yield error messages because R interprets these characters as operators (e.g., “*” indicates multiplication, “&” is a logical conjunction, etc.).\nObject names must not contain spaces. While “item2” is a valid name, “item 2” is not.\n\n\nNot being allowed to use spaces in object names might prevent us from assigning meaningful and intelligible object names. To deal with the issue, there are three common types of notation:\n\nperiod notation: place periods where you would have placed a space, e.g. trial.number\nunderscore notation: replace spaces with underscores, e.g. trial_number\ncamelBack notation: instead of spaces write everything as one word, but write the beginning of new words with a capital letter, e.g. trialNumber\n\nWhich notation you use is purely a matter of preference. You will likely encounter all of them when using R because the people who created base R or additional packages each have their own preferences regarding notation."
  },
  {
    "objectID": "intro4.html",
    "href": "intro4.html",
    "title": "Data frames and lists",
    "section": "",
    "text": "Here, we will look at two types of R objects that we will frequently encounter when working R, data frames and lists. Both of these object types are a little more complex than the ones we talked about so far, but with greater complexity comes greater utility."
  },
  {
    "objectID": "intro4.html#data-frames",
    "href": "intro4.html#data-frames",
    "title": "Data frames and lists",
    "section": "Data frames",
    "text": "Data frames\nData frames are R objects specifically designed for people who work with data. Just like matrices, they are two-dimensional objects. The main difference, however, is that data frames can contain different types of elements. To be exact, data frames are a combination of different vectors of the same length.\nThe general idea of a data frame is to view every row as an observation and every column as a variable. Therefore, each column represents a vector, which also means that, column-wise, all elements must be of the same type. The same is not true for the rows of a data frame.\n\nHow to define a data frame\nWe can define data frames by calling the function data.frame and entering vectors or matrices as function arguments. Here, vectors are treated as column vectors (or nx1 matrices, if you will). The only requirement is that all of the vectors and matrices we want to combine into a data.frame have the same number of rows.\nBelow is very simple example of how we can define a data frame in R. Imagine, we have collected data from 5 participants, which we will identify by the numbers from 1 to 5, and that we asked them to report their gender (open response) and age in years (numeric value).\n\nmy_data_frame = data.frame(\n  c(1, 2, 3, 4, 5),                                       # participant ID\n  c('male', 'female', 'non-binary', 'female', 'female'),  # participant gender\n  c(25, 19, 23, 22, 28)                                   # participant age\n)\n\n\nNote: In this example, the code is distributed across several lines. Technically, we could have written it all in one long line, but doing so makes it very difficult to read the code. Unless we call a function with few or very short function arguments, it is recommended to use one line for each argument.\nAlso note how the function arguments are shifted a bit to the right. This is called indentation. Indentation has no effects on how the code works in R (it does in other programming languages such as Python!). However, it contributes greatly to the legibility of the code by structuring it.\nIt is, therefore, a good idea to get used to organizing longer pieces of code by using multiple lines and proper indentation. Your future self, your collaborators, and third parties, who will read your code when you make it publicly available, will thank you for it.\n\nOnce we execute the piece of code above, the data frame will appear as an object in RStudio’s Environment (top right). We can immediately see that it is a data frame because RStudio describes data frames as x observations of y variables. In our case that is listed as “5 obs. of 3 variables”.\n\nNote: The Environment will show a small light blue button with a white arrowhead to the left of our data frame’s name. If we click this button, RStudio will “unfold” the data frame and show us a list of the vectors it contains as well as the type of the vector.\n\nJust as with matrices, we can have a look at our new object by either entering its name or by clicking on it in the environment. If we do the former, R will return the data frame in the console, whereas the latter will open the data frame in a separate tab next to our scripts. Let’s go with the code version.\n\n\n\n  c.1..2..3..4..5. c..male....female....non.binary....female....female..\n1                1                                                  male\n2                2                                                female\n3                3                                            non-binary\n4                4                                                female\n5                5                                                female\n  c.25..19..23..22..28.\n1                    25\n2                    19\n3                    23\n4                    22\n5                    28\n\n\n\n\n\nAssigning proper variable names\nWe may notice two things: first, the data frame contains exactly the information we specified as function arguments for the data.frame function; second, each column of our data frame has a very weird name. The reason is that data frames require variable names for each column, and if we do not specify these names, R will just use each vector’s content as a name. This is inconvenient for a number of reasons, so it is desirable to assign proper names to the variables.\nOne way to do so is calling the function names, which allows us to change the names of a data frame’s variables. The function ‘names’ requires the name of the data frame as a function argument. We then define it as a character vector containing the new variable names. The character vector must contain one element for each column of the data frame. For our data frame, it could look like this:\n\nnames(my_data_frame) = c('ID', 'gender', 'age')\n\nIf we now ask R to show us the data frame in the console (by entering its name as code), it will look much tidier.\n\n\n\n  ID     gender age\n1  1       male  25\n2  2     female  19\n3  3 non-binary  23\n4  4     female  22\n5  5     female  28\n\n\n\nAnother way to assign sensible variable names is to specify them when creating the data frame. We can do so by making slight adjustments to the way we enter vectors as function arguments when calling the data.frame function. In fact, the code will look as if we define vectors as objects. The only difference is that when doing so within the data.frame function, the vectors will not be created outside the data frame (i.e., they will not appear as separate objects in the Environment). Here is what the code would look like for our data frame.\n\nmy_data_frame = data.frame(  \n  ID = c(1, 2, 3, 4, 5),                                          # ID\n  gender = c('male', 'female', 'non-binary', 'female', 'female'), # gender\n  age = c(25, 19, 23, 22, 28)                                     # age\n) \n\nThe result is the same as if we had used the names function.\n\n\n\n  ID     gender age\n1  1       male  25\n2  2     female  19\n3  3 non-binary  23\n4  4     female  22\n5  5     female  28\n\n\n\n\n\nEntering matrices into data frames\nAs mentioned above, we can also enter matrices as function arguments when creating a data frame, as long as these matrices have the correct number of rows. Let’s assume that we additionally collected responses from two Likert scale items (levels 1 to 5), which we would like to enter into our data frame. Our code would then look like this:\n\nmy_data_frame = data.frame(  \n  ID = c(1, 2, 3, 4, 5),                                          # ID\n  gender = c('male', 'female', 'non-binary', 'female', 'female'), # gender\n  age = c(25, 19, 23, 22, 28),                                    # age\n  matrix(c(1, 3, 5, 4, 3, 5, 5, 3, 2, 2), nrow = 5)               # responses \n) \n\nRunning this code creates a data frame with 5 observations of 5 variables in in the environment. One problem is that we cannot name the two new variables contained in the matrix as we did with the vectors. However, when we look at our data frame, we will notice that the variable names do not look as terrible as they did when we entered the unnamed vectors. Instead, the variables will be named “X1” and “X2” (when entering a matrix, R will just assign the names from “X1” to “Xi” for a matrix with i columns).\nStill, we might prefer less obscure names for our two measures. One possibility is to use the names function as we did above. Another is to create the matrix as a separate object before creating the data frame. This allows us to set proper column names using the function colnames. This function works very similar to the names function but is specifically designed for matrices. Here is what the code would look like.\n\n# First, we create our 5x2 matrix as an object\nresponse_matrix = matrix(\n  c(1, 3, 5, 4, 3, 5, 5, 3, 2, 2), \n  nrow = 5\n)\n\n# Next, we assign column names\ncolnames(response_matrix) = c('item1', 'item2')\n\n# Finally, we create the data frame\nmy_data_frame = data.frame(  \n  ID = c(1, 2, 3, 4, 5),                                          # ID\n  gender = c('male', 'female', 'non-binary', 'female', 'female'), # gender\n  age = c(25, 19, 23, 22, 28),                                    # age\n  response_matrix                                                 # responses \n) \n\nThe new variables in our data frame now have proper names. We can check this by calling the name of our data frame and inspecting it in the console (or by clicking on its name in the Environment).\n\n\n\n  ID     gender age item1 item2\n1  1       male  25     1     5\n2  2     female  19     3     5\n3  3 non-binary  23     5     3\n4  4     female  22     4     2\n5  5     female  28     3     2\n\n\n\n\nDefining objects outside a data frame and then simply using their name as a function argument when calling the data.frame function is not limited to matrices but can also be done for vectors. While there are good reasons for defining objects separately before combining them into a data frame, one downside is that our Environment can get a wee bit cluttered.\nIf we want to tidy up our environment, we can remove objects we no longer need (for example, because we put them into a data frame) using the function rm. We simply need to use the name of the object we want to remove as the function argument for rm, and it will be disappear from the Environment."
  },
  {
    "objectID": "intro4.html#lists",
    "href": "intro4.html#lists",
    "title": "Data frames and lists",
    "section": "Lists",
    "text": "Lists\nThe final type of R object we need to know for now is the list. Lists are very flexible containers that we can create using the list function. Think of lists as multi-purpose storage units. They can contain all other types of R objects (including other lists). For example, we could create a list that contains a single character string, a numeric vector, and a data frame. To do so, we simply call the function list and enter each object we want to store in the list as a function argument.\n\nmy_value = 'hello'          # a single character value\nmy_vector = c(1,1,2,3,5,8)  # a numeric vector\n\n# The following code creates a list with three elements\nmy_list = list(\n  my_value,\n  my_vector,\n  my_data_frame\n)\n\nrm(my_value, my_vector)   # this removes two of the objects from the environment\n\nAs with other objects, the new list will appear in the Environment. RStudio tells us that this object is a list of 3. Just like for a data frame, there is a small light blue button to its left that allows us to unfold the list and have a look at its contents.\nWe can also click on the list in the environment to have a look at it or have R print the list in the console by calling its name. If we do the latter, the output looks like this:\n\n\n\n[[1]]\n[1] \"hello\"\n\n[[2]]\n[1] 1 1 2 3 5 8\n\n[[3]]\n  ID     gender age item1 item2\n1  1       male  25     1     5\n2  2     female  19     3     5\n3  3 non-binary  23     5     3\n4  4     female  22     4     2\n5  5     female  28     3     2\n\n\n\nAs we can see, the output lists the elements of our list one after another. The first element of the list is preceded by “[[1]]”, the second by “[[2]]”, and so on.\n\nNote: Once we start analysing data in R, we will frequently encounter lists. The reason is that many R functions used in inferential statistics use lists as outputs."
  },
  {
    "objectID": "intro5.html",
    "href": "intro5.html",
    "title": "Saving and loading data",
    "section": "",
    "text": "At some point, we either want to load data into R or save a data frame that we have created or modified. Arguably, the best format to store data in tabular form (rows representing observations and columns representing variables) is the CSV (comma-separated variables) format.\nCSV files are ideal for Open Data as most software can read them, including generic text editors.\nIn order to load and save data in CSV format, we can use two functions:"
  },
  {
    "objectID": "intro5.html#saving-data-frames",
    "href": "intro5.html#saving-data-frames",
    "title": "Saving and loading data",
    "section": "Saving data frames",
    "text": "Saving data frames\nIf we save a data frame using the function write.csv, we should specify three function arguments:\n\nx: the name of the object we want to save (a data frame, but works with matrices, too)\nfile: the directory where we want to save the file plus the file name as a character string\nrow.names: whether we want R to assign row names (default is TRUE, but we want to turn that to FALSE)\n\nIf we do not specify a directory, R will save the data frame in the current working directory.\nWe can ask R to tell us the current working directory using the function getwd. This function is special in that it does not have any function arguments. Here is what the syntax looks like:\n\n# Obtain the current working directory\ngetwd()\n\nThe output in the console will look slightly different for each person due to differences in how their folders are named:\n\n\n\n[1] \"C:/Users/Thomas/Documents/R stuff\"\n\n\n\nHere is how the code for saving a data frame could look like.\n\n# save the object called my_data_frame\nwrite.csv(\n  x = my_data_frame,\n  file = \"C:/Users/Thomas/Documents/data.csv\",\n  row.names = FALSE\n)\n\nProvided the directory “C:/Users/Thomas/Documents” already exists on the computer, there will now be a file named “data.csv” in that directory."
  },
  {
    "objectID": "intro5.html#loading-data-frames",
    "href": "intro5.html#loading-data-frames",
    "title": "Saving and loading data",
    "section": "Loading data frames",
    "text": "Loading data frames\nWe can load a data frame (or matrix) saved in CSV format using the function read.csv.\nWe usually need to specify only two function arguments:\n\nfile: the directory and file name of the CSV file\nheader: a Boolean variable indicating whether the first row of file contains the variable names (default is TRUE)\n\n\nImportant: If we simply call the function, R will display the data frame we loaded in the console. If we want it to appear in the Environment we need to define it as a new object.\n\nHere is what the code would look like if we were to load the data frame saved in the previous example.\n\n# load data\nmy_data_frame = read.csv(\n  file = \"C:/Users/Thomas/Documents/data.csv\",\n  header = T)\n\nAssuming that the requested file exists in the specified directory, the Environment will now show the object called “my_data_frame” in the Environment."
  },
  {
    "objectID": "working1.html",
    "href": "working1.html",
    "title": "Binary Operators",
    "section": "",
    "text": "We already know ho to define R objects. We will now turn to the question how to work with them. Generally speaking, we can work with R objects by performing operations on them. There are two ways of doing so: one is to use binary operators. Another way to perform operations on R objects is to feed them into functions as function arguments. Here, we will focus on binary operators."
  },
  {
    "objectID": "working1.html#prelude---the-operator",
    "href": "working1.html#prelude---the-operator",
    "title": "Binary Operators",
    "section": "Prelude - the $ operator",
    "text": "Prelude - the $ operator\nBefore we delve into binary operators, we need to learn about a very useful operator specific to data frames and lists with named elements.\nThis operator is $. It is not a binary operator because it requires only one object.\nSpecifically, the $ operator allows us to extract a named element from a data frame or list.\nTo do so, we need two first write the name of the data frame (or list), immediately follow it with the $ operator, and then follow with the name of the element we want to obtain.\nHere is what the code looks like, generally speaking:\n\ndata_frame$variable_name\n\nHere is an example, in which we obtain a variable called age from a data frame called my_data_frame that, among others, contains this variable.\n\n# create a data frame\nmy_data_frame = data.frame(\n  ID = 1:5,\n  gender = c('m', 'f', 'f', 'nb', 'm'),\n  age = c(32, 66, 37, 41, 38)\n)\n\n# obtain age from the data frame\nmy_data_frame$age\n\nIf we run this code, R will display the information contained in the column labelled age in the console with the output looking as follows:\n\n\n\n[1] 32 66 37 41 38\n\n\n\nWe could also define the extracted column as a separate object and save it in the Environment.\nOne extremely useful feature in R is that we can use the $ operator to create new elements in a data frame or list. For example, we can add a new variable to the data frame from the previous example.\n\n# add a variable to the data frame coding whether \n# the participants is currently employed\nmy_data_frame$employed = c(T, T, F, T, T)\n\nOnce we run the code, R will update the data frame in the Environment. It will now be shown as “5 obs. of 4 variables” instead of the original “5 obs. of 3 variables”. Upon inspection, we can confirm that it now contains the new variable employed as its last column.\nHere is what the data frame would look like in the console:\n\n\n\n  ID gender age employed\n1  1      m  32     TRUE\n2  2      f  66     TRUE\n3  3      f  37    FALSE\n4  4     nb  41     TRUE\n5  5      m  38     TRUE"
  },
  {
    "objectID": "working1.html#back-to-binary-operators",
    "href": "working1.html#back-to-binary-operators",
    "title": "Binary Operators",
    "section": "Back to binary operators",
    "text": "Back to binary operators\nBinary operators are symbols that R uses to represent a specific operation involving two objects. These operations can be arithmetic or logical. Before we jump into action, we need to have a look at how R’s binary operators look like.\n\nArithmetic binary operators\nR has seven built-in arithmetic binary operators (you will probably not use the last two, but we will include them for the sake of completeness).\n\n\n\n\n\n\n\n\nOperator\nOperation\nWhat R does\n\n\n\n\n+\naddition\ncomputes the sum of two numbers\n\n\n-\nsubtraction\nsubtracts the second number from the first\n\n\n*\nmultiplication\ncomputes the product of two numbers\n\n\n/\ndivision\ndivides first number by the second\n\n\n^\npower\ntakes the first to the power of the second\n\n\n%%\nmodulo\ntakes the remainder of division\n\n\n%/%\ninteger division\ndivision rounded down to whole numbers\n\n\n\n\n\nLogical binary operators\nBesides arithmetic operators, R has several built-in logical binary operators. Logical binary operators also require two objects as arguments. They compare the object to the left of the operator to the object on its right and check if the result of this comparison is TRUE or FALSE. In other words, they return a Boolean value. Here is the list of logical operators:\n\n\n\n\n\n\n\nOperator\nWhat R tests\n\n\n\n\n<\nthe first value is less than the second\n\n\n<=\nthe first value is less than or equal to the second\n\n\n>\nthe first value is greater than the second\n\n\n>=\nthe first value is greater than or equal to the second\n\n\n==\nthe first value is exactly equal to the second\n\n\n!=\nthe first value is not equal to the second\n\n\n\n\n\nUsing binary operators on single values\nThe simplest way to use binary operators is to use them on single values. In the case of arithmetic operators, this comes down to adding, subtracting, multiplying etc. two numbers. As the two arguments for the operators, we can use values we defined as objects prior to the operation, values that we enter as is, or a combination of both. Here are a few examples.\n\na = \"hello\"   # a character value\nb = FALSE     # a Boolean value\nd = 13        # a numeric value\ne = 2         # another numeric value\n\na != \"hello\"  # tests if a is unqeual to the string \"hello\" (this is FALSE)\n\nb == FALSE    # tests if b is the Boolean value FALSE (which is TRUE)\n\nd + 3         # adds 3 to the object d (for a total of 10)\n\nd ^ e         # takes d to the power of e (the result is 169)\n\nThis is what appears in the console:\n\n\n\n[1] FALSE\n\n\n[1] TRUE\n\n\n[1] 16\n\n\n[1] 169\n\n\n\n\nNote In the example above, we named the four objects a, b, d, and e. This was neither an oversight nor an expression of dislike toward the letter c. The simple reason is that there is a function called c, and it is prudent to avoid giving objects the same name as existing functions.\nTechnically, it is possible to assign an object the name of a function, but it may lead to confusion or problems with the R code. Therefore, it is best avoided.\n\nWe can create more complex operations by combining multiple arithmetic and/or logical operators involving multiple values, and we can save the result by defining it as another object.\n\nNote that R follows the basic rules of arithmetic operations. That is, power takes precedence over multiplication or division, which, in turn, take precedence over addition or subtraction. Just as in school maths, we need to use parentheses to organize our operations accordingly.\nWe can also use parentheses in the same fashion when combining multiple logical operations.\nWhen combining arithmetic and logical operators, the arithmetic operations take precedence over the logical ones.\n\nExample 1:\n\nx = 3   # define a numeric value\ny = 2   # define another value\n\nz = x ^ 2 / (x * y - x)\n\nIn the example above, we defined a numeric value called z. Since power takes precedence over division, R first computes x to the power of 2 (for a total of 9). It then divides 9 by the expression in the parentheses. Within the parentheses, multiplication takes precedence over subtraction, which means that R first computes the product of x and y (which is 6) and then subtracts x for a total of 3. So our code boils down to dividing 9 by 3. We can easily verify that R did that by inspecting the new object z.\n\nz\n\n\n\n\n[1] 3\n\n\n\nExample 2:\n\nx = 3      # define a numeric value\ny = 2      # define another value\nz = 'red'  # define a character value\n\nz != 'red' & (x > 2 | x + y == 7)\n\nIn this example, we test whether the two conditions combined by & (logical AND) are simultaneously TRUE. First, whether the value z equals the string ‘red’, and second, whether at least one of the following two statements combined with the | (logical OR) is TRUE: the value x is greater than 2 OR the sum of x and y equals 7.\nThe first statement is TRUE because z equals the character string ‘red’. The second statement is also true. While the sum of x and y clearly differs from 7 and is, thus, FALSE, x is greater than 2. Since the logical OR only requires one of the statements to be TRUE, the whole statement in parentheses is TRUE. Therefore, executing this code should return the Boolean TRUE, which we can check by inspecting the output in the console.\nIt looks as follows:\n\n\n[1] TRUE\n\n\n\nIn theory, arithmetic operators should only work on numeric values (integer or double). Accordingly, R will complain if at lest one of the objects we use as arguments is a character string. Specifically, it will return an error message in the console stating that we assigned a non-numeric argument to the binary operator.\nSomething similar should happen if we assign at least one Boolean value, that is, a value stating either a logical TRUE or FALSE. Keep in mind, however, that R sometimes changes the type of an object so that it works with an operator or function (this is called coercion). If we use a Boolean value in an arithmetic operation, R will just treat it as a binary numeric variable (FALSE = 0, TRUE = 1).\n\n\n\nUsing binary operators on vectors and matrices\nWe can also use binary operators on vectors or matrices. The exact operation depends on the two objects involved. More specifically, if one of the two arguments of a binary operator is a vector or a matrix, then the operation differs depending on whether the second argument is a single vector or another vector or matrix.\nLets first look at the (simpler) case where one of the two arguments is a vector/matrix and the other is a single value. In this case, the exact same operation is performed on all elements of the vector or matrix. For example adding a single value and a vector/matrix means that the single value is added to each element of the vector/matrix, multiplying the vector/matrix by a number means that each element is multiplied by that number, testing whether a vector/matrix equals a certain character string tests for each element whether it equals that string, and so on. Let’s look at some examples.\n\nv1 = c('red', 'green', 'blue')  # defines a character vector\n\nm1 = matrix(1:9, nrow = 3)      # defines a numeric 3x3 matrix\n\na = 2         # defines a numeric value\n\nv1 == 'red'   # tests for each element of v1 whether it equals the string 'red'\n\na ^ m1        # takes a (the value 2) to the power of each element of m1\n\nm1 < 5        # tests for each element of m1 whether it is smaller than 5   \n\nThis is what appears in the console:\n\n\n\n[1]  TRUE FALSE FALSE\n\n\n     [,1] [,2] [,3]\n[1,]    2   16  128\n[2,]    4   32  256\n[3,]    8   64  512\n\n\n     [,1]  [,2]  [,3]\n[1,] TRUE  TRUE FALSE\n[2,] TRUE FALSE FALSE\n[3,] TRUE FALSE FALSE\n\n\n\nIrrespective of whether we enter the vector/matrix as the first or the second argument of the binary operator, the output has the same size and dimensions. That is, multiplying a vector of length 4 by a number yields a numeric vector of length 4. Multiplying a 3x3 matrix by a number yields another numeric 3x3 matrix. Testing whether a statement is true for a vector of length 7 returns a Boolean vector of length 7, and so on.\nWe can also use binary operators with both arguments being vectors or matrices. As a general rule, we must use objects of equal size in those operations, that is, if the object to the left of the operator is a vector of length 6, then the object to its right must also be a vector of length 6. Likewise, if we enter a 3x4 matrix to the left of the operator, our argument on its right will also have to be a 3x4 matrix.\n\nCaveat: While R complains when we enter two matrices of unequal size as arguments of a binary operator (it will print an error message in the console), the same is not true for vectors. If we sue binary operators on vectors of unequal length, R will simply extend the shorter vector by starting over.\nFor example, when adding a vector containing the numbers from 1 to 3 to another vector containing the numbers from 1 to 5, R will turn the first vector into a vector of length 5 by repeating the first two elements (the resulting vector then has the elements 1, 2, 3, 1, 2).\nThis built-in feature of R can cause problems because situations, in which we actually want to perform binary operations on vectors of unequal length, are rare. Since R will run the code without any complaints, we run the risk of missing bugs in our code. Therefore, it is prudent to double-check the code whenever we use binary operators with two vectors.\n\nWhen using binary operators with two vectors or matrices, R performs an element-wise operation. That means that R pairs the first element of the first vector or matrix with the corresponding element of the second vector or matrix when performing the operation. The second element of the first object is paired with second element of the second object, and so on. Let’s again look at a few examples.\nVectors first.\n\nv1 = c(2, 4, 6, 8)    # defines a numeric vector\n\nv2 = 1:4              # lazy way of creating a vector of length 4\n\nv1 + v2               # performs element-wise addition of the vectors\n\nv1 != v2              # takes each element of v1 abnd tests whether it is\n                      # different from the corresponding element of v2\n\nThis is what appears in the console:\n\n\n\n[1]  3  6  9 12\n\n\n[1] TRUE TRUE TRUE TRUE\n\n\n\nNow for some matrices.\n\nm1 = matrix(c(2, 4, 6, 8), nrow = 2)    # defines a 2x2 matrix\n\nm2 = matrix(1:4, ncol = 2)              # defines another 2x2 matrix\n\nm1 - m2               # performs element-wise subtraction of the matrices\n\nm1 < m2               # takes each element of m1 tests whether it is less than \n                      # the corresponding element of m2   \n\nThis is what appears in the console:\n\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,] FALSE FALSE\n\n\n\nAs we can see, performing arithmetic operations on matrices using binary operators yields numeric matrices of the same size as the input matrices as result. When using logical binary operators instead, the result will be a Boolean matrix of equal size as the two input matrices.\n\nNote that using the binary operator for multiplication (*) on matrices does not do what we would expect from regular matrix algebra. If we want to do classic matrix multiplication, we need to use a special operator, namely %*%."
  },
  {
    "objectID": "working2.html",
    "href": "working2.html",
    "title": "Functions",
    "section": "",
    "text": "Most of the things we will do when using R will be done using functions. We can think of a function as an (often) elaborate sequence of simple operations. As such, they can involve more than two objects. However, functions are not only more elaborate than operations using a binary operator; they are also more flexible, because we can modify what a function does or how it does it via its additional function arguments."
  },
  {
    "objectID": "working2.html#the-basic-setup-of-r-functions",
    "href": "working2.html#the-basic-setup-of-r-functions",
    "title": "Functions",
    "section": "The basic setup of R functions",
    "text": "The basic setup of R functions\nAll R function share the same setup. The function has a unique name by which we can call it, and it has a number of function arguments that we need to specify in parentheses following the function name when calling the function.\nFunction arguments can be objects that we would like to feed to the function or parameters of the function that influence what the function does. Some of a function’s arguments are required arguments, that is, if we do not specify these arguments, the functions won’t run. Instead, R will complain by printing an error message telling us that we did not specify a required function argument.\nA function may also entail non-required function arguments. Technically, these arguments are also required, but the person who wrote the function defined default values for the arguments. If a function argument has a default value, we do not need to specify it in order for the function to run. The function will simply run as if we had entered the respective argument’s default value manually.\n\nOccasionally, we might come across functions that have no arguments at all. These functions are quite rare. We can can them by writing nothing in the parentheses following the function’s name."
  },
  {
    "objectID": "working2.html#the-most-useful-function-in-r---help",
    "href": "working2.html#the-most-useful-function-in-r---help",
    "title": "Functions",
    "section": "The most useful function in R - help",
    "text": "The most useful function in R - help\nAll functions in R are documented in order to help users understand what the function does, what its arguments are and what they do. We can ask R to show us the documentation using the function help. If we call this function and feed it the name of an R function as a function argument, R will display the documentation in the help tab of the utility & help section (bottom right of RStudio’s interface).\nNote that the function help will accept the function name both if we enter it as is or if we enter it as a character string (i.e., we write it in quotation marks).\nAs an alternative to calling the function’s name in regular R syntax, we can type the functions name in the search bar of the help tab.If we use this way to learn about a function, we do not need quotation marks.\nOnce we call the help function, R will show us the documentation of an existing R function or a group of related functions. R will generally display a lot of information. What information is displayed exactly depends on the function.\nThe following information will be shown for all functions:\n\nthe name of the function\nwhat R package it is from\na description of the the function does\nhow to call the function (usage)\nthe functions’ arguments\na description of the type and format of the function’s output (value)\nfunctioning R code showing examples of how the function can be used\n\nThere may be additional information for some functions such as: - a description of the maths or logic underlying the function (details) - additional information of potential interest to the user (note) - references to literature the literature the function is based on - links to related functions (see also)\nLet’s look at an example by asking R to show us the documentation for the function median using R syntax.\n\nhelp(median)\n\nR will now show us the following in the help tab.\n\n\n\nFig 1. Documentation for the function median\n\n\nLet’s first look at the description. Unsurprisingly, it states that this function computes the sample median.\nOne of most relevant sections of the documentation is the function’s usage. As we can see, the function median has two function arguments, x and na.rm. The second argument, na.rm has a default value, indicated by the equal sign and a specific value (FALSE) to its right. If we do not specify na.rm, R will use the value FALSE as a default. Since the argument x has no default, we must specify it. Otherwise, the function won’t run.\nThe other highly relevant section is the arguments section. It tells us what the function arguments do and values are acceptable for each of the two function arguments. The argument x must be a numeric vector (we can ignore the first part of the sentence referring quite obscurely to ‘an object for which a method has been defined’). This is the set of numbers for which we will compute the median. The second function argument, na.rm, defines how missing values (represented by NA, meaning “not available”) should be handled. The function will not work when our numeric vector x contains at least one NA value. By setting na.rm to TRUE, we can tell R to remove the NA values prior to computing the median.\nFinally, let’s inspect the value section. From what the function is supposed to do, we would expect it to return a single numeric value. The documentation tells us that this value can be a double type value when the vector is of even length (in this case, the median is the mean of the two centremost values of x). It also informs us that the output of the function will be NA if x is either an empty vector of if x contains NA values while na.rm is set to FALSE.\n\nAt the top left of the documentation, R states in braces following the function’s name which R package it stems from. In case of the function median, the package it belongs to is called stats. This package along with base is built into core R.\nCaveat: If a function does not belong to one of R’s built-in packages, R’s help will not know the function unless the respective package is loaded (see the section on installing and loading additional packages for more information)."
  },
  {
    "objectID": "working2.html#finding-functions-if-their-name-is-unknown",
    "href": "working2.html#finding-functions-if-their-name-is-unknown",
    "title": "Functions",
    "section": "Finding functions if their name is unknown",
    "text": "Finding functions if their name is unknown\nSometimes, we might be looking for a function to perform a specific operation, but we do not know whether R has such a function and, if so, how it is called in R. In such situations, we can make use of the search bar on the top right of the help tab. If we input the term we are interested in there, R will return a list of related functions in the help tab.\nLet’s assume, for example, that we are looking for a function that computes the standard deviation of a numeric vector. Entering the term “standard deviation” in the search bar, yields the following output:\n\n\n\nFig 2. Search results for the term “standard deviation”\n\n\nAs we can see, R links to several help pages, each of which contains the documentation for a function that R thinks to relate to our search term. R displays the results of the search in a special format, namely package name::function name. For example, the first result of the search is the function devfun2 of the package lme4. Klicking on any of the links our search returned is equivalent to using the help function for the respective function.\n\nNote that unlike the regular help function, using the search bar is not restricted to currently loaded R packages. Any package that is installed on the computer will be included in the search. This handy feature is designed to maximize the chance of finding the function we are looking for.\n\nIn the example above, we can see that there is a function called sd in the stats package that is part of basic R. The brief description of this function looks promising enough to warrant klicking on the function’s name. The documentation for the function that R now displays confirms that this is the function we are looking for (see below).\n\n\n\nFig 3. Documentation for the function sd\n\n\nAs we can see from the documentations usage and arguments sections, the function sd requires a numeric vector x as a function argument. It has a second optional argument we are already familiar with, namely na.rm, which allows us to exclude NA values prior to computing the standard deviation.\nLet’s have a look at the details section of the function sd. This section contains an important piece of information, namely that the function uses the denominator \\(n-1\\). What this means is that the function sd computes an unbiased estimate of the population standard deviation by multiplying the (uncorrected) sample variance by \\(\\frac{n}{n-1}\\) prior to taking its square root.\nAn alternative to using the search bar of the help tab is to use R code to search for functions. We can do so using a special operator ?? followed by the term we are searching for. If our term consists of more than one word, we need to put it in quotation marks (putting a single word in quotation marks won’t do any harm).\nThe syntax looks like this:\n\n??\"standard deviation\"\n\nWe can verify that searching for our term of interest using R syntax yields the same results in the help tab.\n\nCaveat: There is one situation in which using the search bar of the help tab and using the operator ?? will yield different results. If the search term matches the name of an R function, either from base R or from a currently loaded R package, the help tab will always display the documentation for that function instead of providing us with a list of potentially relevant functions. Using the ?? operator will always display the list of results.\n\n\nIf our search for the desired function is not successful, google (or any non-evil alternative search engine) is our friend. R has a very active and supportive community, and there are extensive resources for R users online."
  },
  {
    "objectID": "working2.html#combining-several-functions",
    "href": "working2.html#combining-several-functions",
    "title": "Functions",
    "section": "Combining several functions",
    "text": "Combining several functions\nWe will frequently find ourselves in situations, in which the operations we want to perform require calling not just one function but a sequence of functions.\nConsider, for example, a case, in which we want to create a list type object that contains descriptive statistics for two variables we observed. The first element of that list will contain the variables’ means while the second will contain their variance-covariance matrix.\nCreating the first element of that list is simple as it requires only computing the means whereas creating th 2x2 matrix requires computing the variances of the two variables as well as their covariance and then putting them in a matrix format.\nThe first way to go about this is to write R code such that we call each function separately and save its output as a new object. We then use this object as a function argument in the next function we need to call. Another possibility is to use a function call as the argument for the next function. Lets have a look at these two options.\n\nCalling functions separately.\n\nv1 = c(2, 3, 5, 7, 9, 11, 13)     # data for variable 1\nv2 = c(1, 1, 2, 3, 5, 8, 13)      # data for variable 2\n\n# compute means for v1 and v2 using the function 'mean'\nmean1 = mean(v1)\nmean2 = mean(v2) \n\n# create a vector of the means using the function 'c'\nmean_vector = c(mean1, mean2)\n\n# compute variances using the function 'var'\nvar1 = var(v1)\nvar2 = var(v2)   \n\n# compute the covariance using the function 'cov'\ncov12 = cov(v1, v2)\n\n# create the variance-covariance matrix using the\n# functions 'matrix' and 'c'\nvc_vector = c(var1, cov12, cov12, var2)\nvc_mat = matrix(vc_vector, nrow = 2)\n\n# now create the list using the function 'list'\nmy_list = list(means = mean_vector,\n               var_cov_matrix = vc_mat)\n\nWe can now have a look at the object we crated by calling the name of our newly defined list.\n\nmy_list\n\n\n\n\n$means\n[1] 7.142857 4.714286\n\n$var_cov_matrix\n         [,1]     [,2]\n[1,] 16.80952 17.04762\n[2,] 17.04762 19.57143\n\n\n\nAs we can see, the objects looks exactly as it should, and it contains the information we want it to contain. There is, however, a drawback to the piece-by-piece approach, we took in creating the list. It becomes apparent when we look at the Environment tab of the Memory section (top right).\n\n\n\nFig 1. Clogged Environment\n\n\nSince we defined the output of each function as an object of its own, our Environment contains not only the list we wanted to create, but also ten additional objects.Two of them are the original data, which we may want to keep, but the other eight objects are no longer required. Eight superfluous objects may not sound too much, but if we work on more complex tasks in R, the number of superfluous objects may increase rapidly.\nOne solution to solve the problem is to clean up afterwards using the function rm. This function takes an arbitrary number of R objects as function arguments. It removes all listed arguments from the Environment. Here is what the code looks like in our example.\n\nrm(mean1, mean2, mean_vector,\n   var1, var2, cov12,\n   vc_vector, vc_mat)\n\nAfter we run this code, the Environment looks much tidier. It now contains only the original data vectors v1 and v2 as well as the list containing the descriptive statistics.\n\n\n\nFig 2. Unclogged Environment\n\n\n\n\nCalling multiple functions in one go\nAn alternative to calling each function separately and defining interim objects is to use function calls as arguments for other functions. In theory, it is possible to write one(very long) line of code to create our list of descriptive statistics from the two data vectors v1 and v2. Here is what the code would look like.\n\nv1 = c(2, 3, 5, 7, 9, 11, 13)     # data for variable 1\nv2 = c(1, 1, 2, 3, 5, 8, 13)      # data for variable 2\n\n# create the list using the function 'list'\nmy_list = list(\n  # create the first element of the list using the \n  # functions 'c' and 'mean'\n  means = c(mean(v1), mean(v2)),\n  \n  # create the second element using the functions \n  # 'matrix', 'c', 'var', and 'cov'\n  var_cov_matrix = matrix(\n    c(var(v1), cov(v1, v2), cov(v1, v2), var(v2)),\n    nrow = 2)\n)\n\nIn the example above, the R code is spread out between multiple lines for the purpose of better readability. We could have actually put it all into one line. If we now look at the Environment, we will only see the two original data vectors and the list we created. No interim objects were created, and there is not need to clean up afterwards.\nNote also how the code starts with the last step of the step-by-step approach. That is, if we use function calls as arguments for other functions, we need to think from the end to the start.\nOne downside of writing code containing convoluted function calls is that the code may become more difficult to parse for others or even for ourselves (especially if we revisit a script we wrote a while ago). For example, if we combine too many function calls, we may end up with a line of code that has a dozen closing brackets at its end. This is not only hard to read, but it may also lead to bugs in our code when we miss a closing bracket or include too many of them.\n\nNote: Either of the two ways of writing code shown above is viable. However, it is often advisable to choose a middle ground, where we define interim objects whenever the code would otherwise become to convoluted.\nAs always, readability of the code greatly benefits from making ample use of commenting."
  },
  {
    "objectID": "working3.html",
    "href": "working3.html",
    "title": "Numerical Indexing",
    "section": "",
    "text": "Most R objects consist of multiple elements, the exception being single values (or vectors of length 1 and 1x1 matrices, which technically are single values, too). There will be situations where we would like to view, extract, or change some, but not all elements of an object. For example we might want to remove the first four cases from a data set because they were test runs, or we might be interested in how the 22nd participant in our recent study responded to questions 13 and 14. What we do in those cases is tell R to look for specific elements of an object. That is called indexing.\nThe most generic form of indexing uses brackets. Specifically, we first write the name of the object of interest. In brackets following the object’s name, we define which elements we want R to obtain. We can obtain elements by their referring to their (numerical) position in an object or via logical operations (using either binary operators or functions).\nNumerical indexing means that we tell R in brackets which elements it should obtain by entering the elements’ position within the object. If the object is a vector, we need only provide a single number per element, wheres we need two coordinates in case our object is two-dimensional (e.g., a matrix or a data frame). Using numerical indexing, we can ask R to obtain a single element but also multiple elements of an R object.\nLets look at a few examples using vectors first."
  },
  {
    "objectID": "working3.html#numerical-indexing-of-vectors",
    "href": "working3.html#numerical-indexing-of-vectors",
    "title": "Numerical Indexing",
    "section": "Numerical indexing of vectors",
    "text": "Numerical indexing of vectors\n\nv1 = c(5, 7, 11, 22, 3, 1, 19) # create a vector with arbitrary numbers\n\n# obtain the 3rd element of vector v1\nv1[3]\n\n# obtain elements 2 to 5 of v1\nv1[c(2,3,4,5)]\n\n# obtain elements 2 to 5 of v1 (lazy/efficient version)\nv1[2:5]\n\n# obtain elements 2, 5, and 7 of v1\nv1[c(2,5,7)]\n\nUsing the code above yields the following output in the R console (because we did not save the obtained elements of v1 as objects of their own, R will print the result in the console).\n\n\n\n[1] 3\n\n\n[1] 2 3 4 5\n\n\n[1]  2  5 NA\n\n\n[1] 2 2\n\n\n[1] 1 2 3\n\n\n\nA neat trick in R is that we can tell it to obtain all elements of an object except for those we specify in brackets. We can do so by preceding the selection of elements with the operator -.\n\n# obtain all elements of v1 except for the 1st and 2nd (3 to 7)\nv1[-(1:2)]\n\n# obtain all elements of v1 except for those (4 to 6)\n# specified in v2\nv1[-v2]\n\nThe output in the console looks like this:\n\n\n\n[1] 3 4 5 6 7\n\n\n[1] 4 5 6 7\n\n\n\nRemember that data frames are essentially a container for column vectors of equal length. Remember also that we can obtain an existing column (i.e., one variable) of a data frame using the $ operator. We can obtain elements of that variable using numerical indexing just as we did in the examples above. Here is what the syntax looks like.\n\n# create a data frame with some demographic data\nmy_df = data.frame(\n  ID = 1:6,                                   \n  age = c(40, 51, 32, 23, 55, 68),\n  gender = c('m', 'f', 'nb', 'f', 'f', 'm'),\n  employment = c(T, T, T, T, T, F)\n  )\n\n# obtain the first element of the age variable\nmy_df$age[1]\n\n# obtain elements 2:4 of the gender variable\nmy_df$gender[2:4]\n\n# obtain all elements of the employment variable \n# except for the first and the last \nmy_df$employment[-c(1, 6)]\n\nHere is what the output in the console would look like:\n\n\n\n[1] 40\n\n\n[1] \"f\"  \"nb\" \"f\" \n\n\n[1] TRUE TRUE TRUE TRUE\n\n\n\nIn some of the examples above, we used vectors in brackets to tell R which elements of the vector v1 we want to obtain. Generally speaking, we can obtain elements of R objects by indexing objects of the same dimensions in brackets, that is, we can use vectors to obtain elements of one-dimensional objects such as vectors and lists, and matrices to obtain elements of two-dimensional objects such as matrices and data frames."
  },
  {
    "objectID": "working3.html#numerical-indexing-of-matrices-and-data-frames",
    "href": "working3.html#numerical-indexing-of-matrices-and-data-frames",
    "title": "Numerical Indexing",
    "section": "Numerical indexing of matrices and data frames",
    "text": "Numerical indexing of matrices and data frames\nLet’s now turn to two-dimensional objects. As mentioned above, we need to tell R the coordinates of the elements we want to obtain in the brackets following the object’s name. The coordinates must be separated by a comma, and we specify the rows before the columns (remember the roman-catholics as a mnemonic aid).\nFor the following examples, we fist generate a 4x4 matrix containing the numbers form 1 to 16.\n\nm1 = matrix(1:16, nrow = 4)  # create a numeric 4x4 matrix\n\nThe matrix looks as follows:\n\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\n\nNow let’s look at few examples of numerical indexing using our matrix m1.\n\n# extract the element in the 2nd row and the 3rd column (10)\nm1[2, 3]\n\n# extract all elements that are in rows 1 and 3 \n# and at the same time in columns 4 and 2\nm1[c(1, 3), c(4, 2)]\n\n# extract elements 1 to 3 in the 2nd row (2, 6, and 10)\nm1[2, 1:3]\n\nHere is what we will see in the console when running the code above.\n\n\n\n[1] 10\n\n\n     [,1] [,2]\n[1,]   13    5\n[2,]   15    7\n\n\n[1]  2  6 10\n\n\n\nA look at the console shows that R returns the desired elements as vectors. The reason is that we either asked for a single value (example 1), two elements from very different parts of the original matrix (example 2), or a vector of neighboring values within the original matrix (example 3).\nIt is possible, though, to obtain elements from a matrix so that R returns another matrix (of smaller size). The output will be another matrix if we extract elements from different rows but the same columns (or vice versa). Let’s have a look at a few examples below:\n\n# extract elements 1 to 3 in the 2nd row (2, 6, and 10)\n# and the 4th row (4, 8, 12)\nm1[c(2, 4), 1:3]\n\n# extract the 3rd and 4th elements of columns 1 (3, 4) \n# and 2 (7, 8)\nm1[3:4, 1:2]\n\nA look at the console confirms, again, that the code worked as intended.\n\n\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n     [,1] [,2]\n[1,]    3    7\n[2,]    4    8\n\n\n\nSometimes we might want to extract all elements of certain rows or columns of a two-dimensional R object. Theoretically, we could do so by entering all rows/columns in brackets, but R has an easier way for us to do that: stating nothing! Yes, you read that correctly. If we do not specify the rows or columns in brackets, R will understand that we want all of them. Since in those cases, the elements will either share their row or column positions, the resulting objects will be two-dimensional again (see examples below).\n\n# extract the complete 1st row (1, 5, 9, 13) and \n# 3rd row (3, 7, 11, 15)\nm1[c(1, 3), ]\n\n# extract the complete 3rd (9 to 12) and 4th column\n# (13 to 16)\nm1[, 3:4]\n\nHere is what R prints in the console:\n\n\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n     [,1] [,2]\n[1,]    3    7\n[2,]    4    8\n\n\n\nFinally, as with vectors, we can use the operator - to tell R that we want all elements of a two-dimensional object except for some of them. For example, we could specify that we want to exclude some rows and some columns in brackets. Since we have two dimensions, we can also combine selection of certain rows with the exclusion of columns and vice versa. Here are a few examples.\n\n# remove the first row but obtain all columns \nm1[-1, ]\n\n# obtain rows 1:3, but remove columns 2 and 4\nm1[1:3, -c(2, 4)]\n\nThe output in the console looks like this:\n\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    3    7   11   15\n[3,]    4    8   12   16\n\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    2   10\n[3,]    3   11\n\n\n\nIn the examples above, we always obtained some elements of a matrix. However, the code would work in the same fashion, had the matrices been data frames instead. To show that this is true, lets look at a few examples using he data frame we created above.\n\n# obtain the element that is in the 3rd row\n# and the 2ns column of the data frame\nmy_df[3, 2]\n\n# obtain the first variable of the data frame\n# (yields the same output as my_df$ID would)\nmy_df[, 1]\n\n# obtain rows 2:4 of the data frame\nmy_df[2:4, ]\n\n# obtain all elements in rows 3 to 6 and\n# in all columns except for the first\nmy_df[3:6, -1]\n\nAs we can see from the output, two-dimensional indexing of data frames works as intended.\n\n\n\n[1] 32\n\n\n[1] 1 2 3 4 5 6\n\n\n  ID age gender employment\n2  2  51      f       TRUE\n3  3  32     nb       TRUE\n4  4  23      f       TRUE\n\n\n  age gender employment\n3  32     nb       TRUE\n4  23      f       TRUE\n5  55      f       TRUE\n6  68      m      FALSE"
  },
  {
    "objectID": "working3.html#numerical-indexing-of-lists",
    "href": "working3.html#numerical-indexing-of-lists",
    "title": "Numerical Indexing",
    "section": "Numerical indexing of lists",
    "text": "Numerical indexing of lists\nFinally, we need to look at how numerical indexing works for lists. At first glance, lists seem to follow the same logic as vectors. They, too, are one-dimensional objects with a certain number of elements. However, indexing of lists uses a slightly different syntax. Specifically, we need to tell R which element or elements of a list we want to obtain using double brackets.\nLet’s first create a simple list to work with. Note how the last element of our list is another list (yes, you can put lists into list, that is how cool R lists are).\n\n# The following code creates a list with four elements\nl1 = list(\n  my_value = 'hello',               # a character value\n  \n  my_vector =  c(1,1,2,3,5,8),      # a numeric vector\n  \n  my_matrix = matrix(NA, nrow = 3,  # a 3x3 matrix of NAs\n                     ncol = 3),      \n  my_list = list(                   # a list of two objects\n    a = c('happy', 'hippo'),\n    b = 42\n  )\n)\n\nHere is what the new list looks like.\n\n\n\n$my_value\n[1] \"hello\"\n\n$my_vector\n[1] 1 1 2 3 5 8\n\n$my_matrix\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n[3,]   NA   NA   NA\n\n$my_list\n$my_list$a\n[1] \"happy\" \"hippo\"\n\n$my_list$b\n[1] 42\n\n\n\nWe can now obtain elements of the list. However, lists are a bit more complicated than the other objects. In fact, there are two ways to obtain elements of a list. The first is using single brackets just as we did it with vectors. If we do that, R will return to us another list containing only the specified elements of the original list.\nIf we instead want to obtain a specific element of a list, we can index that element using double brackets instead of the single ones. Using double brackets will return the element in its original form. It will obtain values as value,s vectors as vector, matrices as matrices and so on. That is, R will not return a list when we use double brackets unless the element we are looking for is another list.\nLet’s look at a few examples using single brackets first (we will do them one by one because the output of a list in the console can be a bit longer).\nWe can obtain a single element.\n\n# obtain a list containing element 2 of the list l1\nl1[2]\n\n\n\n\n$my_vector\n[1] 1 1 2 3 5 8\n\n\n\nWe can obtain multiple elements\n\n# obtain a list containing elements 2 to 4 of the l1\nl1[2:4]\n\n\n\n\n$my_vector\n[1] 1 1 2 3 5 8\n\n$my_matrix\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n[3,]   NA   NA   NA\n\n$my_list\n$my_list$a\n[1] \"happy\" \"hippo\"\n\n$my_list$b\n[1] 42\n\n\n\nWe can exclude some elements using the - operator.\n\n# obtain a list containing all elements of l1 except for the 3rd\nl1[-3]\n\n\n\n\n$my_value\n[1] \"hello\"\n\n$my_vector\n[1] 1 1 2 3 5 8\n\n$my_list\n$my_list$a\n[1] \"happy\" \"hippo\"\n\n$my_list$b\n[1] 42\n\n\n\nLet us now turn to the case where we ant to obtain a specific element of a list. As mentioned above, we will use double brackets to do that. Note that we can only obtain a single element of a list in that fashion. If we try to specify multiple elements in double brackets, R will complain by returning an error message.\n\n# obtain element 2 of the list l1\nl1[[2]]\n\n\n\n\n[1] 1 1 2 3 5 8\n\n\n\nSome elements of our list are indexable elements on their own. When we obtain such an element using double brackets, we can use hierarchical indexing. That is, we can tell R to obtain parts of a part of a list. We can do that by first indexing the element of the list we want to obtain using double brackets and then specifying which element of the obtained element we want R to extract using either single brackets or double brackets depending on whether the desired element is another list or not.\nHere is some example code, in which we extract the first three elements of the second element of our list.\n\n# obtain elements 1 to 3 of the second element of l1\nl1[[2]][1:3]\n\n\n\n\n[1] 1 1 2\n\n\n\nAs a final example (just because we can), lets obtain the second element of the character vector that forms the first element of my_list, which is the last element of l1. In those cases, we need to think backwards (similar to calling multiple functions in one go), that is, we tell R to obtain the fourth element of l1, then to obtain that elements’ first element, and then to obtain that elements’ second element. The code to do so looks as follows:\n\n# obtain the second element of the 1st element of\n# the 4th element of the list l1\nl1[[4]][[1]][2]\n\n\n\n\n[1] \"hippo\"\n\n\n\n\nImportant: Given that indexing of lists is somewhat tricky, you might be wondering why we should bother with it. The answer is that many of the functions we will be using to analyse our data will return lists as their output. These lists can contain tons of information, and we may often require only some of it. Therefore, knowing how to access parts of a list can be extremely handy."
  },
  {
    "objectID": "working3.html#saving-and-overwriting-indexed-elements",
    "href": "working3.html#saving-and-overwriting-indexed-elements",
    "title": "Numerical Indexing",
    "section": "Saving and overwriting indexed elements",
    "text": "Saving and overwriting indexed elements\nSo far, we have only used numerical indexing to make R show us the elements in the console. However, we can do two more things with the obtained elements:\n\nwe can use them as the definition of a new R object\nwe can overwrite them by defining new values for them\n\nIn other words, we can use them as both the left-hand-side and the right-hand-side argument in object definition.\n\nSaving elements of an object as a new object\nLet’s first look at the case in which we want to save parts of an object as an object of its own. For example, we might be interested in creating a reduced data frame, that contains only some of the variables of the original data frame. We will use the data frame defined above in this example.\n\n# create a reduced data frame that omits the \n# employment variable (4th column)\nreduced_df = my_df[, -4]\n\nRunning the code above will create a new object called reduced_df in the Environment. As we can see, it consists of 6 observations of 3 variables as compared to the original data frames 6 observations of 4 variables (which makes perfect sense since we asked R to remove one variable.\n\n\n\nFig 1. Defining part of a data frame as a new object\n\n\nHere is what the new data frame looks like in the console:\n\n\n\n  ID age gender\n1  1  40      m\n2  2  51      f\n3  3  32     nb\n4  4  23      f\n5  5  55      f\n6  6  68      m\n\n\n\n\n\nOverwriting elements of an object\nLet’s now have a look at the second case, in which we use part of an R object as the left-hand-side argument of object definition. As mentioned above, this corresponds to overwriting the existing values of those elements with new values. If we want to overwrite elements of an object, we must consider two rules:\nFirst, the new values must conform with the type of the object. We should overwrite numeric value only with numeric value, character strings with only with other character strings, and so on. If we overwrite existing values with values of different types, R will not return an error message. Instead, it will coerce the object such that all elements are of equal type. For example, trying to overwrite elements of a numeric or Boolean object with a character string will turn the whole object into a character string. If we overwrite an element of a numeric vector with a Boolean value (TRUE or FALSE), the object will remain numeric, and the new values will be interpreted as either 1 or 0, and so on.\n\nIn case of a data frame, each column works as its own object, that is, messing up one variable in this way will usually not mess up the other variables.\nHowever, if we overwrite a complete row of the data frame, we can potentially mess up the whole data frame, because now we tamper with elements of each column.\n\nSecond, we need to tell R the correct number of new values for the elements we want to overwrite. For example, if we want to overwrite two elements of a vector, we need to define the new values in a vector of length two, and if we want to overwrite the elements of a \\(3\\times2\\) matrix, the new values must be defined either in a \\(3\\times2\\) matrix or as a vector of length 6 (in the latter case, R will simply overwrite the old values with the values contained in the vector by filling columns from top to bottom and then moving to the next column). There is an exception to the second rule: we can always specify a single value as the new value. In this case, R will overwrite all of the old values with this new value.\nLet’s look at a few examples using the original data frame we defined above.\n\n# The first participant entered the wrong age; we need to overwrite it with the correct value\nmy_df$age[1] = 41\n\n# We want to capitalise the letters in our gender variable; overwrite the 3rd column (option A)\nmy_df[,3] = c('M', 'F', 'NB', 'F', 'F', 'M')\n\n# overwrite the 3rd column (option B)\nmy_df$gender = c('M', 'F', 'NB', 'F', 'F', 'M')\n\n# Participant 6 wants to retract their data we need to overwrite the 6th row with NAs\nmy_df[6,] = NA\n\nHere is what the data frame looks like after we have redefined the values as per the code above:\n\n\n\n  ID age gender employment\n1  1  41      M       TRUE\n2  2  51      F       TRUE\n3  3  32     NB       TRUE\n4  4  23      F       TRUE\n5  5  55      F       TRUE\n6 NA  NA   <NA>         NA\n\n\n\n\nNote that R has a slightly different way of showing that a value is NA (not available, i.e., missing) in the console whenever the object type is a character string. Rather than simply showing NA as it does when objects are numeric or Boolean, R will instead show  for character strings. This is purely cosmetic, that is, it has no consequences for the way we write code or how R evaluates it.\nIf we inspect the object using RStudio’s viewer, instead, all NA values will be shown as NA irrespective of their type.\n\n\n\nUsing numerical indexing to overwrite variable names\nRemember the names function we used previously to assign new names to the variables in a data fame? This function allows us to obtain the current variable names of a data frame. If we call it and feed it the name of a data frame as its function argument, R will return of character vector containing the names of all column contained in the data frame.\nThe names function also allows us to overwrite the existing variables names if we redefine the object returned by the function call as a character vector of equal length (i.e., one element per column of the data frame). Back then we noticed that using the name function might become tedious if a data frame contained a lot of variables. However, by using numerical indexing, we can circumvent the need to explicitly rename each variable in a data frame.\nSince the name function returns a character vector, we can use numerical indexing to pinpoint the variable names we are interested in. For example, we could ask R to tell us the name of the 2nd variable of a data frame or to overwrite the name of the 4th variable. Here is what the code would look like (we will be using the data frame we worked with above):\n\n# obtain the name of the data frame's 2nd variable\nnames(my_df)[2]\n\n# assign a new name to the 4th variable ('employed' instead of 'employment')\nnames(my_df)[4] = 'employed'\n\nRunning the code above prompts R to return the name of the data frame’s 2nd variable in the console:\n\n\n\n[1] \"age\"\n\n\n\nThis is what the data frame looks like after renaming the 4th column.\n\n\n\n  ID age gender employed\n1  1  41      M     TRUE\n2  2  51      F     TRUE\n3  3  32     NB     TRUE\n4  4  23      F     TRUE\n5  5  55      F     TRUE\n6 NA  NA   <NA>       NA\n\n\n\n\nTechnically, we must place the brackets used for indexing directly after the closing parentheses of the function call. Placing the brackets inside the parentheses instead resulted in error messages in previous versions of R. However, in the more recent versions of R, placing the brackets behind the name of the data frame inside the parentheses of the function call will also work. in other words, the two following lines of code produce identical results:\n\n# obtain the names of the first three variables\nnames(my_df)[1:3]\n\n# alternative code that yields the same result\nnames(my_df[1:3])"
  },
  {
    "objectID": "working4.html",
    "href": "working4.html",
    "title": "Logical Indexing",
    "section": "",
    "text": "So far, we have learned how to obtain or overwrite parts of an R object using numerical indexing. We also know how to obtain a named element of a data frame or list, namely by using the $ operator. We will now turn to logical indexing, that is obtaining elements of objects that satisfy certain conditions.\nJust as with numerical indexing, logical indexing uses brackets. Within these brackets we can specify conditions that be be tested logically such that the result of the test is a TRUE or FALSE statement (i.e., a Boolean variable). R will then only show those elements of the object for which these conditions are true."
  },
  {
    "objectID": "working4.html#logical-indexing-using-binary-operators",
    "href": "working4.html#logical-indexing-using-binary-operators",
    "title": "Logical Indexing",
    "section": "Logical indexing using binary operators",
    "text": "Logical indexing using binary operators\nThe simplest form of logical indexing is to use a single statement involving a logical binary operator. For example, we could ask R to return to us all elements of a numeric vector (including, for example, a column of a data frame) that are greater then 3, or we could ask it to obtain all elements from a Boolean matrix that are TRUE.\nHere are a few examples:\n\n# create a numeric vector\nv1 = 1:6\n\n# create a Boolean 2x3 matrix\nm1 = matrix(\n  c(T, T, F, T, F, F),\n  nrow = 2\n)\n\n# obtain all elements of v1 that are greater than 3\nv1[v1 > 3]\n\n# obtain all elements of m1 that are TRUE\nm1[m1 == T]\n\nHere is what the output in the console looks like:\n\n\n\n[1] 4 5 6\n\n\n[1] TRUE TRUE TRUE\n\n\n\nWe can also obtain elements by combining multiple logical statements using R’s AND and OR operators (& and |, respectively). For example, we could obtain all elements of the numeric vector we created above that exceed 2 AND are smaller than 5.\nIf we combine multiple logical tests in logical indexing, it is important that we tell R in each of the involved tests which variable to test. The following code would not work even though it may seem clear to us what it should mean:\n\n# obtain all elements of v1 that are greater than 2 and smaller than 5\nv1[v1 > 2 & < 5]\n\nWhile it looks as if we asked R to show us all elements that are greater than 2 AND smaller than 5, R will not know which variable the second part of the test refers to and will return an error message (in this message, R will tell us that the < sign following the & operator was unexpected).\nHere is how the code needs to look like if we want to obtain elements of an obect using a combination of multiple logical statements:\n\n# obtain all elements of v1 that are greater than 2 and smaller than 5\nv1[v1 > 2 & v1 < 5]\n\nThis code works as intended, which we can verify by looking at the output in the console.\n\n\n\n[1] 3 4\n\n\n\nOne neat thing about logical indexing is that we are not restricted to referring to the variable we want obtain elements from. As long as the outcome of the logical tests is an object of the same size as the object we want to obtain elements from, the code will work.\nWhat this means is that we can obtain all elements of an object that satisfy conditions in other equally sized objects. This is particularly useful when we want to obtain elements of a data frame. For example, we might want R to show us all elements of one variable that satisfy the condition that another variable equal a specific value. Let’s look at a few specific examples.\n\n# create a data frame containing some demographic variables and responses to three 5-point Likert-scale items\nmy_df = data.frame(\n  ID = 1:6,\n  age = c(23, 21, 18, 16, 19, 17),\n  gender = c('f', 'f', 'm', 'm', 'f', 'f'),\n  item1 = c(2, 5, 5, 4, 5, 3),\n  item2 = c(1, 2, 1, 3, 2, 1),\n  item3 = c(3, 4, 5, 5, 4, 2)\n)\n\n# obtain the responses to item 2 of all participants who are adults\nmy_df$item2[my_df$age >= 18]\n\n# obtain the gender of all participants who scored below 4 on items 1 and 3\nmy_df$gender[my_df$item1 < 4 & my_df$item3 < 4]\n\nHere is what the output in the console looks like:\n\n\n[1] 1 2 1 2\n\n\n[1] \"f\" \"f\"\n\n\nJust as with numerical indexing, we can also use logical indexing to save the obtained elements as a new object of their own, or we can overwrite the obtained elements. Let’s focus on overwriting. Overwriting of values using logical indexing is particularly useful in situations, in which missing data is coded using specific numbers (e.g., -99).\nNumeric values for missing data can cause problems if we do not detect it such as distorting some measures of central tendency (e.g., the mean) or variability (e.g., standard deviation, range). Ideally, we want missing data to be represented by NA when working with R. That means that we may end up having to replace certain values that code missing data with NAs manually. Logical indexing makes this a walk in the park as the following example shows.\n\n# add two more items to the data frame created above, both of which contain missing data coded as -99\nmy_df$item4 = c(3, 5, 3, -99, 4, -99)\nmy_df$item5 = c(-99, 2, 3, 1, -99, 2)\n\n# turn all elements that are equal to -99 into NAs\n# variant A:\nmy_df$item4[my_df$item4 == -99] = NA\nmy_df$item5[my_df$item5 == -99] = NA\n\n# variant B:\nmy_df[my_df == -99] = NA\n\nBoth ways of overwriting the -99s with NAs work. The second one is more parsimonious. However, replacing all elements of a certain value in a data frame can become messy in large data sets. If the code for missing data has been chosen poorly (e.g., 99), there may be some variables for which 999 is an actual and valid value (e.g., a reaction time in milliseconds). Therefore, caution is advised when replacing values for a complete data frame."
  },
  {
    "objectID": "working4.html#logical-indexing-using-functions",
    "href": "working4.html#logical-indexing-using-functions",
    "title": "Logical Indexing",
    "section": "Logical indexing using functions",
    "text": "Logical indexing using functions\nInstead of - or in addition to - binary operators, we can also use functions for logical indexing. We can use functions in two ways here. First, we can use the output of a function call as a value of comparison. For example, rather than testing whether an element of an object exceeds a certain fixed value, we could test whether it exceeds the mean or the median of that object.\nSecond, there a several functions that conduct logical tests. For example, the function is.na tests whether an object is NA, the function is.numeric tests whether an object’s type is numeric (either integer or double), and the function is.finite tests whether a numeric variable has a finite value.\n\nNote: R has a neat way of reversing functions such as is.na, is.numeric, and is.finite. We can make R test the respective opposite by preceding the function call with an exclamation mark (similar to how the operator != mean not equal).\nFor example, calling !is.na tests whether an object is not missing, !is.numeric tests whether the object’s type is different from numeric, and !is.finite tests whether a numeric value is infinite (or Inf in R terms).\n\nLet’s look at a few examples of logical indexing using functions. We will do so with the data frame we created and modified above. Here is a quick update on how the data frame looks currently.\n\n\n\n  ID age gender item1 item2 item3 item4 item5\n1  1  23      f     2     1     3     3    NA\n2  2  21      f     5     2     4     5     2\n3  3  18      m     5     1     5     3     3\n4  4  16      m     4     3     5    NA     1\n5  5  19      f     5     2     4     4    NA\n6  6  17      f     3     1     2    NA     2\n\n\n\nWe will now obtain and overwrite some of the elements of this data frame using the is.na function and its inverse, !is.na. Note that the complete function call including the opening and closing parentheses must be contained within the brackets.\n\n# obtain the age of all participants whose responded to item4\nmy_df$age[!is.na(my_df$item4)]\n\n# replace missing responses to items 4 with the average response (i.e., the mean of valid responses to that item across participants) \nmy_df$item5[is.na(my_df$item5)] = mean(my_df$item5, na.rm = T)\n\nRunning the first line of code above, yields the following output in the console:\n\n\n\n[1] 23 21 18 19\n\n\n\nAnd here is what the data frame looks like after we replaced the missing responses with the average response.\n\n\n\n  ID age gender item1 item2 item3 item4 item5\n1  1  23      f     2     1     3  3.00     2\n2  2  21      f     5     2     4  5.00     2\n3  3  18      m     5     1     5  3.00     3\n4  4  16      m     4     3     5  3.75     1\n5  5  19      f     5     2     4  4.00     2\n6  6  17      f     3     1     2  3.75     2\n\n\n\n\nNote how R now displays two decimals for item4. It does not display decimals for the other items, even though their object type is double - just as that of item4 (we can find out the object types by calling the function typeof and feeding it the column of interest as the sole function argument).\nR generally only uses as many decimals as it has to. If a double type object contains only whole numbers, then R will simply omit the decimals."
  },
  {
    "objectID": "working5.html",
    "href": "working5.html",
    "title": "R Packages",
    "section": "",
    "text": "Base R comes equipped with a range of useful functions. However, we might frequently find ourselves in situations in which base R does not provide solutions to our problems, or where the provided solutions are cumbersome. There is a solution to this problem: additional R packages."
  },
  {
    "objectID": "working5.html#what-are-r-packages",
    "href": "working5.html#what-are-r-packages",
    "title": "R Packages",
    "section": "What are R packages?",
    "text": "What are R packages?\nR packages are a collection of R functions and/or R objects (such as data sets) not included in base R. They have been developed and are being maintained by people (often other academics) who believe that their package allows R users to do something that base R cannot do or to make life easier for users compared to using base R alone.\nR is modular in the sense that we can add as many packages to base R as we want. Due to the large R community, chances are that if you look for something specific there is an R package for it.\nMost R packages are available from CRAN (Comprehensive R Archive Network). CRAN is a repository for R packages that ensures that the packages meet certain standards such as being propely documented, interfering as little as possible with other packages, being (largely) platform-independent and so on."
  },
  {
    "objectID": "working5.html#installing-r-packages",
    "href": "working5.html#installing-r-packages",
    "title": "R Packages",
    "section": "Installing R packages",
    "text": "Installing R packages\nIn order to use an R package, we first need to install it on our computer. Installing an R package means that we download it from CRAN. We do not need to do anything else because R will do the actual installing on its own. Once we have downloaded an R package, our instance of R knows that this package exists on our machine. We can then find functions from that package when searching for functions using the ?? operator.\nWhen using RStudio, we can install R packages in two ways. The first is calling the function install.packages and feeding it the name of the R package we want to install as a character string (or as a character vector if we want to install multiple packages). Here is what the syntax looks like:\n\n# install an R package\ninstall.packages(\"packageName\")\n\nAlternatively, RStudio allows us to install packages using the Packages tab in the Utility & Help (bottom right) section of the interface. When we go to this tab, R will show us all the packages we have currently installed (see below). If we do not yet have any additional packages, we will only see packages that are part of base R such as base, datasets, and methods.\n\nKlicking on the Install button will open a menu that allows us to select the package we are interested in for installation (see below).\n\nThe first field is called “Install from”. The default for the installation of packages is to install them from CRAN (this is sensible, we are rarely - if at all - going to install packages not contained in CRAN).\nIn the next field, called “Packages”, we can enter the name of the package we want to install. In case we want to install multiple packages, we can separate their names with space or comma. Note that RStudio has an auto-complete feature, here. That is, if we enter one or more letters or numbers, RStudio will list all packages whose names start with (or consist of) what we have entered so far.\nUnder “Install to Library”, RStudio shows the folder, in which our R packages will be saved (it depends solely on where you installed R on your computer). Once, we hit the “Install” button on the bottom of the menu, R will install the selected packages.\n\nOn the bottom of the menu, there is a check-box asking whether RStudio should install dependencies. This box ix checked by default, and we should leave it that way. The reason is that some R packages depend on other R packages to run properly because they use some of the functions those other packages provide. Those other packages might themselves require other packages to run properly.\nInstalling dependencies, thus, is necessary to ensure that we can actually use the functions that an R package offers us.\n\nLet’s now look at an example, in which we are interested in installing the R package psych because it contains some functions that are particularly interesting to psychologists. We can either install it using R code, which would look like this:\n\n# install the R package psych\ninstall.packages('psych')\n\nAlternatively, we can install the package psych by going to the Packages tab of RStudio’s Utility & Help section, and entering its name in the Install menu (see below).\n\nIrrespective of how we tell R to install the psych package, R will now download the package and unpack it in the folder where R keeps all the packages. We can verify that R has installed the new package by inspecting the output in the console. There, R will provide some information, including the statement that the package we wanted to install has been ‘successfully unpacked’. In our example, it looks as follows:\n\n\nInstalling R packages is something we need to do only once (the exception is when there is a major update of R - then we need to reinstall all packages).\nTherefore, it is not a good idea to include the install.packages function in an R script. If we do, then R will - without need - reinstall packages whenever we run the script.\nThere may be some cases, in which we may want to include the installation of packages in our R script, for example, when we publish our script as part of Open Data and want to make sure that our code is 100% reproducible.\nA common and sensible practice in such cases is to use commenting to include the R code for installing the packages but have R ignore it while running the script. Interested parties who want to run our script, can then simply uncomment the respective lines of code to install the packages."
  },
  {
    "objectID": "working5.html#loading-installed-libraries",
    "href": "working5.html#loading-installed-libraries",
    "title": "R Packages",
    "section": "Loading installed libraries",
    "text": "Loading installed libraries\nIn order to actually use the objects and/or functions included in a package, installing it is not sufficient. We also need to load the package. Once we load an R package, we can access its content until we close RStudio. This also means that we need to load R packages again after closing and restarting RStudio.\nWe can load R packages using the function library which takes the name of an installed R package as its function argument either as is or as a character string (calling the package name as is might be preferable because of RStudio’s code completion feature).\nFor example, if we wanted to us the psych package we installed in the example above, we would need to load it using the library function. The syntax looks as follows:\n\n# load the library psych\nlibrary(psych)\n\n# alternative code that does the same\nlibrary('psych')\n\nRunning either line of code will load the R package. If the package we are interested in has dependencies, loading it will also prompt R to load all other packages necessary for the focal package to run properly. One the package (and potentially its dependencies) are loaded, we can start using its contained functions and/or objects."
  },
  {
    "objectID": "working5.html#updating-libraries",
    "href": "working5.html#updating-libraries",
    "title": "R Packages",
    "section": "Updating libraries",
    "text": "Updating libraries\nJust as R and RStudio, (most) R packages are regularly updated. Using an outdated version of an R package is usually not a big problem. However, in some cases, it may lead to our code not running properly. This can happen whenever a function depends on functions from base R or another R package that have been modified such they have a different name or that their output format has changed.\nIn some instances, R will let us know that our packages are outdated, namely if a package was built in an older version of R than the one we are currently running on our machine. If we load a library that was built using a previous R version, we will receive a warning in the console upon loading is with the libraries function.\n\nJust as error messages, warnings are displayed as red font in the console. However, while error messages tell us that the code we entered did not work, the code does run properly when we receive a warning. Warnings are a way for R to tell us that we may potentially - but not necessarily - run into problems.\n\nRStudio has a quality-of-life feature that allows us to check whether there are updates available for the R packages we have currently installed. In the Packages tab of the Utility & Help section, there is a button called Update (see below).\n\nKlicking on the Update button will open a list of packages for which a more recent version is available. We can then select those packages we would like to update or tell RStudio to update all of them. once we have selected at least one package that requires updating, we can hit the button labelled Install Updates.\nWhat R will do is simply reinstall the package’s current version and overwrite the old version on our computer. In case all of our packages are up to date, RStudio will show us a message stating so."
  },
  {
    "objectID": "working5.html#conflicts-between-packages",
    "href": "working5.html#conflicts-between-packages",
    "title": "R Packages",
    "section": "Conflicts between packages",
    "text": "Conflicts between packages\nA final thing to consider when using R packages is that sometimes there can be conflicts between them, namely when they contain objects with identical names.\nFor example, both the ggplot2 package, which many people use for plotting in R has a function called alpha (regulates colour transparency in plots). So does the psych package (computes Cronbach’s \\(\\alpha\\)). If we load packages with such name conflicts, one of them will dominate the other.\nWhen loading a package that contains objects and/or functions with the same name as functions and/or objects of packages already loaded, R will ‘mask’ similarly named objects from already loaded packages. If a function or object is masked, we will not be able to call it in the usual manner because R will instead call the funciton or object of similar name in the most recently loaded package.\nFor example, if we first load the package psych, and then load ggplot2, R will tell us that the function alpha has been masked from psych. If we call alpha now, R will refer to the ggplot2 version of alpha and assume that we want to manipulate the transparency of a colour. Has we first loaded ggplot2 and then loaded psych, R would instead tell us that alpha has been masked from ggplot2, and calling the function would allow us to compute Cronbach’s \\(\\alpha\\).\nThe question now is: what if I need both functions or objects sharing the same name? The answer is that there we can specify which package we are calling a function from. This is called namespacing. To do so, we need to precede the function name with the package name and separate the two using double colons. The function of the double colon is not unlike that of the $ operator. It tells R where to look for the function we are interested in. The syntax looks as follows:\n\n# call function from package by namespacing it\npackageName::functionName(arguments)\n\nFor example, when loading both the ggplot2 and psych packages, we can call both functions using the syntax above. Doing so will ensure that our code will run free of bugs no matter in which order we load the packages.\n\n# call function alpha from ggplot2 and set the\n# transparency of the colour 'blue' to 50% (0.5)\nggplot2::alpha('blue', alpha = 0.5)\n\n# call the function alpha from psych and\n# compute Cronbach's alpha for a numeric \n# matrix containing responses to a scale\npsych::alpha(x = data_matrix)\n\n\nFun fact: The notation with double colons also allows us to call functions from packages that are not currently loaded."
  },
  {
    "objectID": "working_x.html",
    "href": "working_x.html",
    "title": "Custom functions",
    "section": "",
    "text": "Sometimes, we may not find an R function that does exactly what we want it to do in base R, and we may also run out of luck when browsing additional R packages in search od that function. In those cases, we can resort to writing our own functions using a special function called function (yes, very creative, I know).\nWriting R functions is easy, once we understand how R functions work (and we do know that from looking at the documentation of different functions). What we need to do when defining a custom function is to: - give it a name (ideally a unique name to avoid conflicts with existing functions) - tell R what function arguments the function should have - tell R whether any of the arguments should have default values - tell R what operations it should perform when we call the function\nWhen defining the function, we need to use a specific Syntax. Just as with R objects, we write the name of our custom function to the left of an equal sign. To its right we call the function function. This function does not have fixed arguments that we specifiy in parentheses. Instead, we write those arguments in parentheses that we want our our custom function to have. If we would like there to be a default value for a function argument, we can use an equal sign following the argument name and enter the respective value to its right.\nFinally, we use braces to define what the function should do, that is, which operations to run on the arguments we fed it. Here, sky is the limit to our creativity. We can use any binary operator or function (even other custom functions) to tell R what it it should do. The final line of code within the braces should be a call of the function return, which we can use to define the output of our custom function. Let’s look at a few examples.\n\nExample 1\nIn the first example, we will write a custom function that computed the uncorrected standard deviation of a numeric vector (remember that the function sd that is built into base R computed the bias-corrected standard deviation, which may not always be what we are looking for). We will call this function sd_uncorrected (any name is fine, but it may be prudent to choose a name that is informative of what the function does).\nThe R code for defining this function could look as follows:\n\nsd_uncorrected = function(x, na.rm = TRUE){\n  \n  x = na.omit(x)  # excludes elements of x that are NA (if any)\n  n = length(x)   # sample size n is the length of the vector x\n  \n  var_x = 1/n * sum((x - mean(x))^2)  # compute uncorrected variance of x\n  sd = sqrt(var_x)                    # take square root to get sd\n  \n  return(sd)                          # return the object sd, which happens to be\n                                      # a single numeric value\n  \n}"
  }
]