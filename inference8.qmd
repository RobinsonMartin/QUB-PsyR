---
title: "Multiple Regression"
---

So far,w e have considered linear regression models with a single predictor. We will now turn to cses with two or more predcitors. As soon as a regression model has more then one predictor, we speak of **multiple regression**. 

Multiple regression is a straightforward extension of the simple linear regression. Generally speaking the regression model looks like this:

$$Y = b_0 + b_1X_1+b_2X_2+...+b_kX_k +\epsilon$$
Here, $b_0$ is the intercept, $X_1$ to $X_k$ are the predcitors, and $b_1$ to $b_k$ are their respective regression weights. As always, $\epsilon$ is the residual. We can test each parameter of our model for significance via a $t$-test:

$$\frac{b_i - \beta_i}{SE_{b_i}} \sim t_{N-k}$$

Just as in simple linear regression, we compute the difference between the observed regresison parameter and its expected value under $H_0$ and divide this difference by the parameters' standard error $SE_{b_i}$. The resulting variable follows a $t$-distrbutiobn with $N-k$ degrees of freedom where $k$ is the number of parameters.

Just as in simple linear regression, $\hat{Y}$ is the predcition of our model:

$$\hat{Y} = b_0 + b_1X_1+b_2X_2+...+b_jXj$$
Since we can compute $\hat{Y}$, we can also compute the coefficient of determination $R^2$ and its adjusted version using the exact same formulae. The only conceptual difference is that - since we now have more than one predictor - $R^2$ is no longer the square of a ordinary correlation coefficient. For this reason $R$ (note the capital R) is also referred to as the **multiple correlation coefficient**. Importantly, however, this does not change how we interpret $R^2$. It is still our measure of the proportion of variance of your criterion $Y$ explained by our regression model as a whole. The statistical test of $R^2$ remains unchanged, as well, that is we test for significance of $R^2$ using an $F$-distribution with $k-1$ numerator degrees of freedom and $N-k$ denominator degrees of freedom.

The function we will use to do multiple regression analysis is the same we used for simple linear regression, namely the function *lm*. The only difference is that we now specify the formula argument of the *lm* function such that it contains two or more predictors.


### Excurse: formula objects with multiple predictors

Before we delve into multiple regression in R, we first need to understand how to define the formula object if we have several predictors. Let's first look at cases, in which we want to predict $Y$ from two predictors $X_1$ and $X_2$

```{r}
#| eval: false
#| echo: true
# formula treating y as an additive function of x1 and x2
y ~ x1 + x2
```

In the example above, we have two additive effects (think of them as main effects) of $X_1$ and $X_2$ on $Y$. But what if we wanted to add an interaction as well? In this case, we can combine the two predictors using the **:** operator. If we use this operator, R will compute the interaction term as the product of the two variables and use it as a predictor in the model. Here is what the syntax would look like:

```{r}
#| eval: false
#| echo: true
# formula treating y as a function of x1 and x2
# as well as their interaction
y ~ x1 + x2 + x1:x2
```

In the above example, we have the **full model**, that is, it contains all possible effects that a regression model with two predictors can have. However, it is also possible to feed a reduced model into the *lm* function. For example, we might want to estimate a regression model that entails only the main effect of $X_1$ and the interaction effect, but not the main effect of $X_2$. In this case, the model would look like this:

```{r}
#| eval: false
#| echo: true
# formula treating y as an additive function of x1 
# and the interaction of x1 and x2
y ~ x1 + x1:x2
```

R has a neat way of making regression model formulae more parsimonious, namely using the **\*** operator. If we combine tow or more variables with this operator, R will include all main effects and interactions of these variables. Here are some examples:

```{r}
#| eval: false
#| echo: true
#| code-overflow: wrap
## formula for a full regression model with two predictors
# this:
y ~ x1*x2

# reads as this:
y ~ x1 + x2 + x1:x2


## formula for a full regression model with three predictors
# this:
y ~ x1*x2*x3

# reads as this:
y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3


## formula for a three-predictor models with the 
## third predictor as a purely additive effect
# this:
y ~ x1*x2 + x3

# reads as this:
y ~ x1 + x2 + x1:x2 + x3
```

As we can see, the **\*** operator can make our lives a bit easier whenever models involve multiple predictors. 


### Multiple Regression in R

Let's now look at the syntax for a multiple regression model. We will use some made-up data to run a regression model with two predictors $X_1$and $X_2$. Here is what  the data look like:

```{r}
#| eval: true
#| echo: false
suppressMessages(library(MASS))
set.seed(1234)
tmp = round(mvrnorm(100, mu = c(0, 0, 0), 
                    Sigma = matrix(c(1, .4, .4, .4, 1, .1, .4, .1, 1), nrow = 3)), 2)

df1 = data.frame(ID = 1:100, Y = tmp[,1], X1 = tmp[,2], X2 = tmp[,3])

head(df1)
```

We now have to make a choice: we can either predict $Y$ from $X-1$ and $X_2$ in a purely additive model, or we can include the interaction term $X_1 \times X_2$ as a third predictor in the model. Let's first run the simpler model that omits the interaction effect. Here is the syntax:

```{r}
#| eval: false
#| echo: true
# predict Y from X1 and X2 in a multiple regression
mod1 = lm(formula = Y ~ X1 + X2, data = df1)

# display the results
summary(mod1)
```

Once we run that code, R will produce the following console output:

:::{.alert .alert-warning}
```{r}
#| eval: true
#| evho: false
# predict Y from X1 and X2 in a multiple regression
mod1 = lm(formula = Y ~ X1 + X2, data = df1)

# display the results
summary(mod1)

```
:::

As we can see, the section of the output names "coefficients" contains information on the intercept and the regression weights for the two predictors. In this example both of them are significantly related to the criterion $Y$. Note that since we have three parameters in the model and the total sample size $N$ is 100, the $t$-tests for each parameter have 97 degrees of freedom, and the $F$-test of $R^2$ against zero has 2 numerator and 97 denominator degrees of freedom. 

How do we interpret the regression model? The answer is: pretty much like a simple linear regression. The intercept $b_0$ indicates the level of $\hat{Y}$ (that is, the predictor level of $Y$) when both $X_1$ and $X_2$ are fixed at zero. Since we did not model and interaction of the two predictors, the regression weights $b_1$ and $b_2$ are straightforward to interpret: $b_1$ tells us much $\hat{Y}$ increases when we increase $X_1$ by one unit, irrespective of the current level of $X_2$; $b_2$ tells us the same for increases in $X_2$. 

Let's now look at the full model that includes the interaction of $X_1$ and $X_2$ as a third predictor. As explained above, there are two ways to define the formula object:

```{r}
#| eval: false
#| echo: true
# predict Y from X1, X2, and their interaction
# Version A:
mod2 = lm(formula = Y ~ X1 + X2 + X1:X2, data = df1)

# Version B:
mod2 = lm(formula = Y ~ X1*X2, data = df1)

# display the results
summary(mod2)
```

Running the code above yields the following output in the console:

:::{.alert .alert-warning}
```{r}
#| eval: true
#| echo: false
# predict Y from X1, X2, and their interaction
# Version A:
mod2 = lm(formula = Y ~ X1 + X2 + X1:X2, data = df1)

# Version B:
mod2 = lm(formula = Y ~ X1*X2, data = df1)

# display the results
summary(mod2)
```
:::

The output now shows four parameters in the "coefficients" section. Adding a third predictor cost us one degree of freedom for both the individual $t$-tests of the model parameters and and $F$-test of $R^2$ when compared with the previous model.

The interpretation of the model parameters is the same as before for the main effects of $X_1$ and $X_2$. Since the interaction term is simply another predictor from the vantage point of the model, it indicates how much $\hat{Y}$ increases if we increase the product of $X_1$ and $X_2$ by one unit. While this statement is mathematically true, it is often difficult to grasp what these products represent conceptually (e.g., what is the product of one's experienced stress and available resources?). The important thing to keep in mind about interaction terms in multiple regression is the following: if this interaction is statistically significant, we ~~know~~ decide to believe that the magnitude of the relation of $X_1$ and $Y$ depends on the level of $X_2$ (and that the strength of the relation of $X_2$ and $Y$ depends on the level of $X_1$). 


### Hierarchical regression

As soon as we have more than one predictor variable, we can think about comparing **nested** regression models. We speak of nested models if one model is an extension of another. To be precise, a regression model $A$ is nested in another model $B$, when model $A$ contains only some of the predictors of model $B$ without adding new ones. In hierarchical linear regression, we compare nested models such that we start with a simple model and add predictors in each step.

As long as the earlier models in a hierarchical regression are nested in the later models, we can test whether adding a predictor (or a set of predictors) improves the overall fit of the model. We already know how to measure the overall predictive power of a regression model, namely via the variance explained by a regression model. The respective measure is the coefficient of determination $R^2$. Since we expand models in a hierarchical regression analysis, the proportion of the variance of $Y$ explained by the second model is always equal to or greater than that explained by the first model. The question is whether the increase in $R^2$ is large enough to justify the addition of predictors. 

How can we test whether an increase in $R^2$ between two nested models is statistically significant? We first need to remember that sums of squares follow $\chi^2$-distributions. Next, we need to remember that the difference between two $\chi^2$-distributed variables is itself $\chi^2$-distributed. The degrees of freedom of the difference variables equals the difference of the degrees of freedoms of the two $\chi^2$-distributed variables it was computed from. If we, for example computed the difference between a $\chi^2$-distributed variable $A$ with 10 degrees of freedom and another $\chi^2$-distributed variable $B$ with 8 degrees of freedom, the resulting variable $A-B$ would follow a $\chi^2$-distribution with 2 degrees of freedom. 

To test whether the difference between the $R^2$ of two nested models is different, we now need to remember that $R^2$ is computed from **sums of squares**, which are $\chi^2$-distributed. 

$$R^2 = \frac{S_{\hat{Y}}}{S_{Y}} = \frac{SS_{regression}}{SS_{total}} = 1-\frac{SS_{residual}}{SS_{total}}$$

If we compare two nested models, the denominator will be the same because it represents the variance of the observed criterion $Y$. What will differ is the variance of the model prediction $S_{\hat{Y}}$. Therefore, the models will also differ regarding their $SS_{regression}$ and $SS_{residual}$. In order to compare the two models' respective fit, we first need to compute the difference between their respective $SS_{regression}$ or their $SS_{residual}$ to obtain a $\chi^2$-distributed variable (since $SS_{residual} = SS_{total} - SS_{regression}$, the difference will be the same). We will alls this variable $SS_\Delta$. Since the $SS_{regression}$ have $N-k$ parameters, we can easily compute the degrees of freedom of $SS_\Delta$:

$$df_{SS_\Delta} = N-k_{model1} - (N-k_{model2})= N-k_{model1}-N+k_{model2}=k_{model2}-k_{model1}$$

now that we have the difference of the sums of squares $SS_\Delta$ and its degrees of freedom, the last step is to compute an $F$-statistic to test whether it represent a statistically significant increase in explained variance. This statistic looks as follows:

$$\frac{ \frac{SS_\Delta}{df_{SS_\Delta}} }{\frac{SS_{residual_{model2}}}{df_{residual_{model2}}}} \sim F_{df_{SS_\Delta}; df_{residual_{model2}}}$$

now that we know ho to test for a significant $\Delta R^2$ on a conceptual level, the question is how to run this test in R. We can do so easily using the function *anova*. The *anova* function is similar to the *summary* function in that it can be used on a range of different models an will perform different operations and produce different outputs based on the objects exact nature. If we call the *anova* function and feed it two or more nested regression models, it run a significance test of $\Delta R2$ between the first and second model, between the second and third model, and so on.

Let's now have look at the syntax. We will use the two regression models we looked at above because they are nested (*mod1* is nested within *mod2* because it lacks the interaction effect). It is important to enter the models in ascending order of complexity (simplest model first). The code will run even if we enter more complex model first, but we may add up with negative values for the test statistic, which makes no sense mathematically.

```{r}
#| eval: false
#| echo: true
#| code-overflow: wrap
# test for a significant increase in variance explained
# between the less complex mod1 and the more complex mod2
anova(mod1, mod2)
```

Here is the console output:

:::{.alert .alert-warning}
```{r}
#| eval: true
#| echo: false
# test for a significant increase in variance explained
# between the less complex mod1 and the more complex mod2
anova(mod1, mod2)
```
:::

The output first displays the formulae of the two models we compared. More important is the table following it. As we can see, the *anova* function computes $SS_{\Delta}$ as the difference between the two models' $SS_{residual}$. It also shows the models' respective degrees of freedom as well as the degrees of freedom for the model difference $SS_{\Delta}$. Since we only added one parameter in model 2 (the regression weight for the interaction term), this difference has one degree of freedom. Finally, the function computes the $F$-statistic for the significance test and reports the associated $p$-value. in our case, the test is significant, meaning that the second model explains a significantly greater proportion of the variance of $Y$.  

:::{.alert .alert-info}
If you look at the $p$-value for $\Delta R^2$, you will notice that it is identical to the $p$-value of the significance test of the regression weight for the interaction term in the summary of *mod2*. Since we only added one variable, and this variable is a significant predictor of $Y$, it follows that the model must explain a significantly larger proportion of the variance of $Y$. 

In other words, if we add a single predictor in a hierarchical regression analysis, we can test its significance using either the $t$-test on its regression weight or the $F$-test on $\Delta R^2$. The result will be the same.
:::


### Multiple regression with categorical predictors

Just as in a simple linear regression, our predictors can be categorical instead of continuous. Here, we will focus on the case with one continuous predictor $X_1$ and one categorical predictor $X_2$. Let's first assume that $X_2$ is dichotomous and that we use it as a dummy-coded predictor (coded 0 vs. 1). Here is some made-up data.

```{r}
#| echo: false
#| eval: true
# generate some data
set.seed(1234)
X1 = round(rnorm(50, mean = 10, sd = 2),2)
X2 = rep(c(0, 1), 25)
Y = round(5 + X1*X2 + rnorm(50, 0, 1),2)

# put the data into a data frame
df2 = data.frame(ID = 1:50, Y, X1, X2)

# show the data 
head(df2)
```

First, we will run a regression model containing only the two main effects. The syntax is the same as for continuous predictors:

```{r}
#| eval: false
#| echo: true
#| code-overflow: wrap
# run the regression analysis
mod3a = lm(formula = Y ~ X1 + X2, data = df2)

# show the results
summary(mod3a)
```

Here is the console output:

:::{.alert .alert-warning}
```{r}
#| eval: true
#| echo: false
# run the regression analysis
mod3a = lm(formula = Y ~ X1 + X2, data = df2)

# show the results
summary(mod3a)
```
:::

As we can see, both predictors are significant. Time to interpret the model parameters! The intercept tells us the estimated value $\hat{Y}$ for observations in the reference category ($X_2 = 0$) when $X_1$ is zero. The regression weight of $X_1$ indicates that for each increase in $X_1$ our estimate $\hat{Y}$ increases by roughly 0.47 units, while the respective coefficient for $X_2$ states that moving from the reference category to the other category ($X_2 = 1$) is associated with an increase in $\hat{Y}$ of roughly 9.18 points. So far, so good.






